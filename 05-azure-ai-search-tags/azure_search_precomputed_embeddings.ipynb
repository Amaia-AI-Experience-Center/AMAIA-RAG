{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Search con Embeddings Pre-calculados\n",
    "\n",
    "Este notebook utiliza los datos generados en el módulo anterior (04-document-ingestion) que ya incluyen:\n",
    "- Embeddings vectoriales\n",
    "- Categorías\n",
    "- Tags\n",
    "\n",
    "A diferencia del enfoque con Skillset, aquí NO necesitamos generar embeddings durante la ingesta,\n",
    "simplemente indexamos los vectores que ya existen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from __future__ import annotations\n\nimport json\nimport os\nimport pathlib\nimport time\nfrom typing import Any, Dict, List\n\nfrom azure.core.credentials import AzureKeyCredential\nfrom azure.core.exceptions import ResourceExistsError\nfrom azure.storage.blob import BlobServiceClient\n\nfrom azure.search.documents import SearchClient\nfrom azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\nfrom azure.search.documents.models import VectorizedQuery\nfrom azure.search.documents.indexes.models import (\n    HnswAlgorithmConfiguration,\n    IndexingParameters,\n    SearchField,\n    SearchFieldDataType,\n    SearchIndex,\n    SearchIndexer,\n    SearchIndexerDataContainer,\n    SearchIndexerDataSourceConnection,\n    SimpleField,\n    SearchableField,\n    VectorSearch,\n    VectorSearchProfile,\n)\nfrom dotenv import load_dotenv\nfrom openai import AzureOpenAI"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Configuración\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "load_dotenv(override=True)\n\n# Azure Search\nAZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\nAZURE_SEARCH_KEY = os.getenv(\"AZURE_SEARCH_KEY\")\n\n# Azure Storage\nAZURE_STORAGE_CONNECTION_STRING = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\nBLOB_CONTAINER_NAME = os.getenv(\"AZURE_STORAGE_CONTAINER\", \"documents-precomputed-vectors\")\nBLOB_PREFIX = \"json-chunks\"\n\n# Resource names\nINDEX_NAME = \"chunks-precomputed-vectors-index\"\nDATA_SOURCE_NAME = \"chunks-blob-datasource\"\nINDEXER_NAME = \"chunks-blob-indexer\"\nVECTOR_PROFILE_NAME = \"chunksProfile\"\nALGORITHM_NAME = \"chunksHnsw\"\n\n# Data file from previous module\nDATA_FILE = pathlib.Path(\"..\") / \"04-document-ingestion\" / \"data\" / \"rag_ingested_chunks.json\"\n\nprint(\"Configuración cargada\")\nprint(f\"  - Archivo de datos: {DATA_FILE}\")\nprint(f\"  - Índice: {INDEX_NAME}\")\nprint(f\"  - Contenedor: {BLOB_CONTAINER_NAME}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Inicializar Clientes\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Validate environment variables\nrequired_vars = [\n    \"AZURE_SEARCH_ENDPOINT\",\n    \"AZURE_SEARCH_KEY\",\n    \"AZURE_STORAGE_CONNECTION_STRING\",\n    \"AZURE_OPENAI_API_KEY\",\n    \"AZURE_OPENAI_ENDPOINT\"\n]\n\nmissing = [var for var in required_vars if os.getenv(var) is None]\nif missing:\n    raise EnvironmentError(f\"Faltan variables de entorno: {', '.join(missing)}\")\n\n# Initialize Azure Search clients\ncredential = AzureKeyCredential(AZURE_SEARCH_KEY)\nindex_client = SearchIndexClient(endpoint=AZURE_SEARCH_ENDPOINT, credential=credential)\nindexer_client = SearchIndexerClient(endpoint=AZURE_SEARCH_ENDPOINT, credential=credential)\nsearch_client = SearchClient(endpoint=AZURE_SEARCH_ENDPOINT, index_name=INDEX_NAME, credential=credential)\n\n# Azure OpenAI client for generating embeddings in queries\nopenai_client = AzureOpenAI(\n    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n)\nEMBEDDING_DEPLOYMENT = os.environ[\"AZURE_OPENAI_DEPLOYMENT_EMBEDDING\"]\n\nprint(\"Clientes inicializados\")\nprint(f\"Modelo de embeddings: {EMBEDDING_DEPLOYMENT}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Cargar Documentos con Embeddings Pre-calculados\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not DATA_FILE.exists():\n    raise FileNotFoundError(f\"No se encuentra el archivo: {DATA_FILE}\")\n\nwith open(DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n    documents = json.load(f)\n\nprint(f\"Cargados {len(documents)} documentos\")\nprint(f\"\\nEjemplo del primer documento:\")\nsample_doc = documents[0].copy()\n# Truncate embedding for visualization\nif 'embedding' in sample_doc:\n    embedding_len = len(sample_doc['embedding'])\n    sample_doc['embedding'] = f\"[vector de {embedding_len} dimensiones]\"\nprint(json.dumps(sample_doc, indent=2, ensure_ascii=False))\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Subir Documentos a Blob Storage\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nSubiendo documentos al contenedor de Blob Storage...\")\nblob_service = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)\ncontainer_client = blob_service.get_container_client(BLOB_CONTAINER_NAME)\n\ntry:\n    container_client.create_container()\n    print(f\"  Contenedor '{BLOB_CONTAINER_NAME}' creado\")\nexcept ResourceExistsError:\n    print(f\"  Contenedor '{BLOB_CONTAINER_NAME}' ya existe, se reutiliza\")\n\nuploaded_count = 0\nfor doc in documents:\n    blob_name = f\"{BLOB_PREFIX}/{doc['id']}.json\"\n    data = json.dumps(doc, ensure_ascii=False).encode(\"utf-8\")\n    container_client.upload_blob(name=blob_name, data=data, overwrite=True)\n    uploaded_count += 1\n    if uploaded_count % 100 == 0:\n        print(f\"  Subidos {uploaded_count}/{len(documents)} documentos...\")\n\nprint(f\"  {uploaded_count} documentos subidos correctamente\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Crear Índice de Azure AI Search\n",
    "\n",
    "El índice acepta vectores pre-calculados. No necesitamos vectorizer para la ingesta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nCreando índice para vectores pre-calculados...\")\n\n# Define index fields\nfields = [\n    SimpleField(\n        name=\"id\", \n        type=SearchFieldDataType.String, \n        key=True, \n        filterable=True, \n        sortable=True\n    ),\n    SearchableField(\n        name=\"content\", \n        type=SearchFieldDataType.String, \n        searchable=True\n    ),\n    SimpleField(\n        name=\"category\", \n        type=SearchFieldDataType.String, \n        filterable=True, \n        facetable=True, \n        sortable=True\n    ),\n    SimpleField(\n        name=\"source\", \n        type=SearchFieldDataType.String, \n        filterable=True, \n        facetable=True\n    ),\n    SimpleField(\n        name=\"tags\",\n        type=SearchFieldDataType.Collection(SearchFieldDataType.String),\n        filterable=True,\n        facetable=True,\n    ),\n    SearchField(\n        name=\"embedding\",\n        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n        searchable=True,\n        vector_search_dimensions=1536,\n        vector_search_profile_name=VECTOR_PROFILE_NAME,\n    ),\n]\n\n# Configure vector search\nvector_search = VectorSearch(\n    profiles=[\n        VectorSearchProfile(\n            name=VECTOR_PROFILE_NAME,\n            algorithm_configuration_name=ALGORITHM_NAME,\n        )\n    ],\n    algorithms=[HnswAlgorithmConfiguration(name=ALGORITHM_NAME)],\n)\n\nindex = SearchIndex(name=INDEX_NAME, fields=fields, vector_search=vector_search)\n\n# Delete previous index if exists\ntry:\n    index_client.delete_index(INDEX_NAME)\n    print(\"  Índice anterior eliminado\")\nexcept Exception:\n    pass\n\nindex_client.create_index(index)\nprint(f\"  Índice '{INDEX_NAME}' creado\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Crear Data Source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nCreando data source del contenedor...\")\n\n# Preventive cleanup of indexer\ntry:\n    indexer_client.delete_indexer(INDEXER_NAME)\nexcept Exception:\n    pass\n\ncontainer = SearchIndexerDataContainer(name=BLOB_CONTAINER_NAME, query=f\"{BLOB_PREFIX}\")\ndata_source = SearchIndexerDataSourceConnection(\n    name=DATA_SOURCE_NAME,\n    type=\"azureblob\",\n    connection_string=AZURE_STORAGE_CONNECTION_STRING,\n    container=container,\n    description=\"Blob container con chunks y embeddings pre-calculados\",\n)\n\ntry:\n    indexer_client.delete_data_source_connection(DATA_SOURCE_NAME)\nexcept Exception:\n    pass\n\nindexer_client.create_data_source_connection(data_source)\nprint(f\"  Data source '{DATA_SOURCE_NAME}' creada\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Crear Indexer (SIN Skillset)\n",
    "\n",
    "Como los embeddings ya están calculados, NO necesitamos un skillset.\n",
    "El indexer simplemente lee los campos del JSON y los mapea al índice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nCreando indexer (sin skillset, vectores pre-calculados)...\")\n\nindexer = SearchIndexer(\n    name=INDEXER_NAME,\n    data_source_name=DATA_SOURCE_NAME,\n    target_index_name=INDEX_NAME,\n    description=\"Indexer para chunks con embeddings pre-calculados\",\n    parameters=IndexingParameters(configuration={\"parsingMode\": \"json\"}),\n)\n\ntry:\n    indexer_client.delete_indexer(INDEXER_NAME)\nexcept Exception:\n    pass\n\nindexer_client.create_indexer(indexer)\nprint(f\"  Indexer '{INDEXER_NAME}' creado\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n▶️  Ejecutando indexer y esperando a que finalice...\")\n",
    "indexer_client.run_indexer(INDEXER_NAME)\n",
    "\n",
    "timeout_seconds = 300\n",
    "waited = 0\n",
    "poll_interval = 5\n",
    "\n",
    "while waited < timeout_seconds:\n",
    "    status = indexer_client.get_indexer_status(INDEXER_NAME)\n",
    "    execution_history = status.execution_history\n",
    "    current_status = status.status\n",
    "    last_result = status.last_result\n",
    "    \n",
    "    # Mostrar estado actual\n",
    "    print(f\"  • Estado del indexer: {current_status} (esperados {waited}/{timeout_seconds}s)\")\n",
    "    \n",
    "    # Si hay un resultado de ejecución, mostrarlo\n",
    "    if last_result:\n",
    "        print(f\"    - Estado última ejecución: {last_result.status}\")\n",
    "        print(f\"    - Items procesados: {last_result.item_count if last_result.item_count else 0}\")\n",
    "        print(f\"    - Items fallidos: {last_result.failed_item_count if last_result.failed_item_count else 0}\")\n",
    "        \n",
    "        # Mostrar errores si existen\n",
    "        if last_result.errors:\n",
    "            print(f\"    - Errores detectados:\")\n",
    "            for error in last_result.errors[:3]:  # Mostrar los primeros 3\n",
    "                print(f\"      * {error.error_message}\")\n",
    "                if hasattr(error, 'key'):\n",
    "                    print(f\"        Documento: {error.key}\")\n",
    "        \n",
    "        # Mostrar warnings si existen\n",
    "        if last_result.warnings:\n",
    "            print(f\"    - Warnings: {len(last_result.warnings)}\")\n",
    "            for warning in last_result.warnings[:2]:\n",
    "                print(f\"      * {warning.message if hasattr(warning, 'message') else str(warning)}\")\n",
    "        \n",
    "        # Verificar si completó con éxito\n",
    "        if last_result.status == \"success\":\n",
    "            print(f\"\\n  ✓ Indexer completado exitosamente\")\n",
    "            break\n",
    "        elif last_result.status == \"transientFailure\":\n",
    "            print(f\"    ⚠️  Fallo transitorio, continuará reintentando\")\n",
    "        elif last_result.status == \"error\":\n",
    "            print(f\"\\n  ❌ Error permanente: {last_result.error_message}\")\n",
    "            raise RuntimeError(f\"Indexer falló: {last_result.error_message}\")\n",
    "    \n",
    "    time.sleep(poll_interval)\n",
    "    waited += poll_interval\n",
    "else:\n",
    "    # Timeout alcanzado - mostrar información detallada\n",
    "    print(f\"\\n  ❌ Timeout alcanzado después de {timeout_seconds}s\")\n",
    "    print(f\"\\n  Información de diagnóstico:\")\n",
    "    print(f\"    - Estado final: {current_status}\")\n",
    "    if last_result:\n",
    "        print(f\"    - Última ejecución: {last_result.status}\")\n",
    "        if last_result.errors:\n",
    "            print(f\"    - Errores encontrados: {len(last_result.errors)}\")\n",
    "    raise TimeoutError(\"El indexer no completó dentro del tiempo esperado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ejecutar Indexer y Esperar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nEjecutando indexer y esperando a que finalice...\")\nindexer_client.run_indexer(INDEXER_NAME)\n\ntimeout_seconds = 300\nwaited = 0\npoll_interval = 5\n\nwhile waited < timeout_seconds:\n    status = indexer_client.get_indexer_status(INDEXER_NAME)\n    last_result = status.last_result\n    \n    if last_result and last_result.status == \"success\":\n        print(f\"  Indexer completado exitosamente\")\n        print(f\"    - Documentos procesados: {last_result.item_count}\")\n        print(f\"    - Documentos fallidos: {last_result.failed_item_count}\")\n        if last_result.failed_item_count > 0:\n            print(f\"    Errores: {last_result.error_message}\")\n        break\n    elif last_result and last_result.status == \"transientFailure\":\n        print(f\"  Fallo transitorio: {last_result.error_message}\")\n    elif last_result and last_result.status == \"error\":\n        raise RuntimeError(f\"Indexer falló: {last_result.error_message}\")\n    \n    print(f\"  Estado: {status.status} (esperados {waited}/{timeout_seconds}s)\")\n    time.sleep(poll_interval)\n    waited += poll_interval\nelse:\n    raise TimeoutError(\"El indexer no completó dentro del tiempo esperado\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Función auxiliar para generar embeddings de queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_query_embedding(text: str) -> List[float]:\n    \"\"\"\n    Generate embedding for a query using Azure OpenAI.\n    \"\"\"\n    response = openai_client.embeddings.create(\n        model=EMBEDDING_DEPLOYMENT,\n        input=text\n    )\n    return response.data[0].embedding\n\nprint(\"Función de embedding lista\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Búsqueda Vectorial Pura\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def vector_search(query: str, top_k: int = 3) -> None:\n    print(f\"\\nBúsqueda vectorial: '{query}'\")\n    \n    # Generate query embedding\n    query_vector = generate_query_embedding(query)\n    \n    # Create vectorized query\n    vector_query = VectorizedQuery(\n        vector=query_vector,\n        k_nearest_neighbors=top_k,\n        fields=\"embedding\"\n    )\n    \n    # Execute search\n    results = search_client.search(\n        search_text=None,\n        vector_queries=[vector_query],\n        select=[\"id\", \"content\", \"category\", \"tags\"],\n        top=top_k\n    )\n    \n    print(f\"\\nResultados:\")\n    found = False\n    for i, result in enumerate(results, 1):\n        found = True\n        snippet = result[\"content\"][:150].replace(\"\\n\", \" \")\n        score = result.get(\"@search.score\", 0.0)\n        category = result.get(\"category\", \"N/A\")\n        tags = result.get(\"tags\", [])\n        \n        print(f\"\\n{i}. Score: {score:.4f}\")\n        print(f\"   ID: {result['id']}\")\n        print(f\"   Categoría: {category}\")\n        print(f\"   Tags: {', '.join(tags) if tags else 'N/A'}\")\n        print(f\"   Contenido: {snippet}...\")\n    \n    if not found:\n        print(\"  (No se encontraron resultados)\")\n\n# Perform example searches\nvector_search(\"¿Qué museos puedo visitar en Bilbao?\")\nvector_search(\"Gastronomía y pintxos\")\nvector_search(\"Cómo llegar y transporte\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Búsqueda Híbrida (Texto + Vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def hybrid_search(query: str, top_k: int = 3) -> None:\n    print(f\"\\nBúsqueda híbrida (texto + vector): '{query}'\")\n    \n    # Generate query embedding\n    query_vector = generate_query_embedding(query)\n    \n    # Create vectorized query\n    vector_query = VectorizedQuery(\n        vector=query_vector,\n        k_nearest_neighbors=top_k,\n        fields=\"embedding\"\n    )\n    \n    # Execute hybrid search (text + vector)\n    results = search_client.search(\n        search_text=query,  # Text search also\n        vector_queries=[vector_query],\n        select=[\"id\", \"content\", \"category\", \"tags\"],\n        top=top_k\n    )\n    \n    print(f\"\\nResultados:\")\n    found = False\n    for i, result in enumerate(results, 1):\n        found = True\n        snippet = result[\"content\"][:150].replace(\"\\n\", \" \")\n        score = result.get(\"@search.score\", 0.0)\n        category = result.get(\"category\", \"N/A\")\n        tags = result.get(\"tags\", [])\n        \n        print(f\"\\n{i}. Score: {score:.4f}\")\n        print(f\"   ID: {result['id']}\")\n        print(f\"   Categoría: {category}\")\n        print(f\"   Tags: {', '.join(tags) if tags else 'N/A'}\")\n        print(f\"   Contenido: {snippet}...\")\n    \n    if not found:\n        print(\"  (No se encontraron resultados)\")\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Búsqueda con Filtros (por categoría y tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def filtered_search(query: str, category: str = None, tags: List[str] = None, top_k: int = 3) -> None:\n    filters = []\n    if category:\n        filters.append(f\"category eq '{category}'\")\n    if tags:\n        tag_filters = [f\"tags/any(t: t eq '{tag}')\" for tag in tags]\n        filters.extend(tag_filters)\n    \n    filter_str = \" and \".join(filters) if filters else None\n    \n    print(f\"\\nBúsqueda con filtros: '{query}'\")\n    if filter_str:\n        print(f\"   Filtros: {filter_str}\")\n    \n    # Generate query embedding\n    query_vector = generate_query_embedding(query)\n    \n    # Create vectorized query\n    vector_query = VectorizedQuery(\n        vector=query_vector,\n        k_nearest_neighbors=50,  # Search more candidates before filtering\n        fields=\"embedding\"\n    )\n    \n    # Execute search with filters\n    results = search_client.search(\n        search_text=query,\n        vector_queries=[vector_query],\n        filter=filter_str,\n        select=[\"id\", \"content\", \"category\", \"tags\"],\n        top=top_k\n    )\n    \n    print(f\"\\nResultados:\")\n    found = False\n    for i, result in enumerate(results, 1):\n        found = True\n        snippet = result[\"content\"][:150].replace(\"\\n\", \" \")\n        score = result.get(\"@search.score\", 0.0)\n        category = result.get(\"category\", \"N/A\")\n        tags = result.get(\"tags\", [])\n        \n        print(f\"\\n{i}. Score: {score:.4f}\")\n        print(f\"   ID: {result['id']}\")\n        print(f\"   Categoría: {category}\")\n        print(f\"   Tags: {', '.join(tags) if tags else 'N/A'}\")\n        print(f\"   Contenido: {snippet}...\")\n    \n    if not found:\n        print(\"  (No se encontraron resultados)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Ejemplos de búsqueda con filtros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_search(\"información turística\", category=\"Bilbao\")\n",
    "\n",
    "filtered_search(\"museos\", tags=[\"museos\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Realizar búsquedas híbridas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_search(\"Guggenheim\")\n",
    "\n",
    "hybrid_search(\"running y deportes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}