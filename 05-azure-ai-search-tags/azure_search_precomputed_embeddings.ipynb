{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Search con Embeddings Pre-calculados\n",
    "\n",
    "Este notebook utiliza los datos generados en el módulo anterior (04-document-ingestion) que ya incluyen:\n",
    "- Embeddings vectoriales\n",
    "- Categorías\n",
    "- Tags\n",
    "\n",
    "Aquí NO necesitamos generar embeddings durante la ingesta, simplemente indexamos los vectores que ya existen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.exceptions import ResourceExistsError\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from azure.search.documents.indexes.models import (\n",
    "    HnswAlgorithmConfiguration,\n",
    "    IndexingParameters,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndex,\n",
    "    SearchIndexer,\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataSourceConnection,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    VectorSearch,\n",
    "    VectorSearchProfile,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuración\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "# Azure Search\n",
    "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "AZURE_SEARCH_KEY = os.getenv(\"AZURE_SEARCH_KEY\")\n",
    "\n",
    "# Azure Storage\n",
    "AZURE_STORAGE_CONNECTION_STRING = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "BLOB_CONTAINER_NAME = os.getenv(\"AZURE_STORAGE_CONTAINER\", \"documents-precomputed-vectors\")\n",
    "BLOB_PREFIX = \"json-chunks\"\n",
    "\n",
    "# Resource names\n",
    "INDEX_NAME = \"chunks-precomputed-vectors-index\"\n",
    "DATA_SOURCE_NAME = \"chunks-blob-datasource\"\n",
    "INDEXER_NAME = \"chunks-blob-indexer\"\n",
    "VECTOR_PROFILE_NAME = \"chunksProfile\"\n",
    "ALGORITHM_NAME = \"chunksHnsw\"\n",
    "\n",
    "# Data file from previous module\n",
    "DATA_FILE = pathlib.Path(\"..\") / \"04-document-ingestion\" / \"data\" / \"rag_ingested_chunks.json\"\n",
    "\n",
    "print(\"Configuración cargada\")\n",
    "print(f\"  - Archivo de datos: {DATA_FILE}\")\n",
    "print(f\"  - Índice: {INDEX_NAME}\")\n",
    "print(f\"  - Contenedor: {BLOB_CONTAINER_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inicializar Clientes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate environment variables\n",
    "required_vars = [\n",
    "    \"AZURE_SEARCH_ENDPOINT\",\n",
    "    \"AZURE_SEARCH_KEY\",\n",
    "    \"AZURE_STORAGE_CONNECTION_STRING\",\n",
    "    \"AZURE_OPENAI_API_KEY\",\n",
    "    \"AZURE_OPENAI_ENDPOINT\",\n",
    "    \"AZURE_OPENAI_ENDPOINT_US\"\n",
    "]\n",
    "\n",
    "missing = [var for var in required_vars if os.getenv(var) is None]\n",
    "if missing:\n",
    "    raise EnvironmentError(f\"Faltan variables de entorno: {', '.join(missing)}\")\n",
    "\n",
    "# Initialize Azure Search clients\n",
    "credential = AzureKeyCredential(AZURE_SEARCH_KEY)\n",
    "index_client = SearchIndexClient(endpoint=AZURE_SEARCH_ENDPOINT, credential=credential)\n",
    "indexer_client = SearchIndexerClient(endpoint=AZURE_SEARCH_ENDPOINT, credential=credential)\n",
    "search_client = SearchClient(endpoint=AZURE_SEARCH_ENDPOINT, index_name=INDEX_NAME, credential=credential)\n",
    "\n",
    "# Initialize Azure OpenAI client for chat (US endpoint)\n",
    "chat_client = AzureOpenAI(\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT_US\"]\n",
    ")\n",
    "\n",
    "# Initialize Azure OpenAI client for embeddings (default endpoint)\n",
    "embeddings_client = AzureOpenAI(\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    ")\n",
    "\n",
    "EMBEDDING_DEPLOYMENT = os.environ[\"AZURE_OPENAI_DEPLOYMENT_EMBEDDING\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cargar Documentos con Embeddings Pre-calculados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATA_FILE.exists():\n",
    "    raise FileNotFoundError(f\"No se encuentra el archivo: {DATA_FILE}\")\n",
    "\n",
    "with open(DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "print(f\"Cargados {len(documents)} documentos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Subir Documentos a Blob Storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSubiendo documentos al contenedor de Blob Storage...\")\n",
    "blob_service = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)\n",
    "container_client = blob_service.get_container_client(BLOB_CONTAINER_NAME)\n",
    "\n",
    "try:\n",
    "    container_client.create_container()\n",
    "    print(f\"  Contenedor '{BLOB_CONTAINER_NAME}' creado\")\n",
    "except ResourceExistsError:\n",
    "    print(f\"  Contenedor '{BLOB_CONTAINER_NAME}' ya existe, se reutiliza\")\n",
    "\n",
    "uploaded_count = 0\n",
    "for doc in documents:\n",
    "    blob_name = f\"{BLOB_PREFIX}/{doc['id']}.json\"\n",
    "    data = json.dumps(doc, ensure_ascii=False).encode(\"utf-8\")\n",
    "    container_client.upload_blob(name=blob_name, data=data, overwrite=True)\n",
    "    uploaded_count += 1\n",
    "    if uploaded_count % 100 == 0:\n",
    "        print(f\"  Subidos {uploaded_count}/{len(documents)} documentos...\")\n",
    "\n",
    "print(f\"  {uploaded_count} documentos subidos correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Crear Índice de Azure AI Search\n",
    "\n",
    "El índice acepta vectores pre-calculados. No necesitamos vectorizer para la ingesta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreando índice para vectores pre-calculados...\")\n",
    "\n",
    "# Define index fields\n",
    "fields = [\n",
    "    SimpleField(\n",
    "        name=\"id\", \n",
    "        type=SearchFieldDataType.String, \n",
    "        key=True, \n",
    "        filterable=True, \n",
    "        sortable=True\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"content\", \n",
    "        type=SearchFieldDataType.String, \n",
    "        searchable=True\n",
    "    ),\n",
    "    SimpleField(\n",
    "        name=\"category\", \n",
    "        type=SearchFieldDataType.String, \n",
    "        filterable=True, \n",
    "        facetable=True, \n",
    "        sortable=True\n",
    "    ),\n",
    "    SimpleField(\n",
    "        name=\"source\", \n",
    "        type=SearchFieldDataType.String, \n",
    "        filterable=True, \n",
    "        facetable=True\n",
    "    ),\n",
    "    SimpleField(\n",
    "        name=\"tags\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.String),\n",
    "        filterable=True,\n",
    "        facetable=True,\n",
    "    ),\n",
    "    SearchField(\n",
    "        name=\"embedding\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=1536,\n",
    "        vector_search_profile_name=VECTOR_PROFILE_NAME,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Configure vector search\n",
    "vector_search = VectorSearch(\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=VECTOR_PROFILE_NAME,\n",
    "            algorithm_configuration_name=ALGORITHM_NAME,\n",
    "        )\n",
    "    ],\n",
    "    algorithms=[HnswAlgorithmConfiguration(name=ALGORITHM_NAME)],\n",
    ")\n",
    "\n",
    "index = SearchIndex(name=INDEX_NAME, fields=fields, vector_search=vector_search)\n",
    "\n",
    "# Delete previous index if exists\n",
    "try:\n",
    "    index_client.delete_index(INDEX_NAME)\n",
    "    print(\"  Índice anterior eliminado\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "index_client.create_index(index)\n",
    "print(f\"  Índice '{INDEX_NAME}' creado\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Crear Data Source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreando data source del contenedor...\")\n",
    "\n",
    "# Preventive cleanup of indexer\n",
    "try:\n",
    "    indexer_client.delete_indexer(INDEXER_NAME)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "container = SearchIndexerDataContainer(name=BLOB_CONTAINER_NAME, query=f\"{BLOB_PREFIX}\")\n",
    "data_source = SearchIndexerDataSourceConnection(\n",
    "    name=DATA_SOURCE_NAME,\n",
    "    type=\"azureblob\",\n",
    "    connection_string=AZURE_STORAGE_CONNECTION_STRING,\n",
    "    container=container,\n",
    "    description=\"Blob container con chunks y embeddings pre-calculados\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    indexer_client.delete_data_source_connection(DATA_SOURCE_NAME)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "indexer_client.create_data_source_connection(data_source)\n",
    "print(f\"  Data source '{DATA_SOURCE_NAME}' creada\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Crear Indexer (SIN Skillset)\n",
    "\n",
    "Como los embeddings ya están calculados, NO necesitamos un skillset.\n",
    "El indexer simplemente lee los campos del JSON y los mapea al índice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreando indexer (sin skillset, vectores pre-calculados)...\")\n",
    "\n",
    "indexer = SearchIndexer(\n",
    "    name=INDEXER_NAME,\n",
    "    data_source_name=DATA_SOURCE_NAME,\n",
    "    target_index_name=INDEX_NAME,\n",
    "    description=\"Indexer para chunks con embeddings pre-calculados\",\n",
    "    parameters=IndexingParameters(configuration={\"parsingMode\": \"json\"}),\n",
    ")\n",
    "\n",
    "try:\n",
    "    indexer_client.delete_indexer(INDEXER_NAME)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "indexer_client.create_indexer(indexer)\n",
    "print(f\"  Indexer '{INDEXER_NAME}' creado\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ejecutar Indexer y Esperar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEjecutando indexer y esperando a que finalice...\")\n",
    "indexer_client.run_indexer(INDEXER_NAME)\n",
    "\n",
    "timeout_seconds = 300\n",
    "waited = 0\n",
    "poll_interval = 5\n",
    "\n",
    "while waited < timeout_seconds:\n",
    "    status = indexer_client.get_indexer_status(INDEXER_NAME)\n",
    "    last_result = status.last_result\n",
    "    \n",
    "    if last_result and last_result.status == \"success\":\n",
    "        print(f\"  Indexer completado exitosamente\")\n",
    "        print(f\"    - Documentos procesados: {last_result.item_count}\")\n",
    "        print(f\"    - Documentos fallidos: {last_result.failed_item_count}\")\n",
    "        if last_result.failed_item_count > 0:\n",
    "            print(f\"    Errores: {last_result.error_message}\")\n",
    "        break\n",
    "    elif last_result and last_result.status == \"transientFailure\":\n",
    "        print(f\"  Fallo transitorio: {last_result.error_message}\")\n",
    "    elif last_result and last_result.status == \"error\":\n",
    "        raise RuntimeError(f\"Indexer falló: {last_result.error_message}\")\n",
    "    \n",
    "    print(f\"  Estado: {status.status} (esperados {waited}/{timeout_seconds}s)\")\n",
    "    time.sleep(poll_interval)\n",
    "    waited += poll_interval\n",
    "else:\n",
    "    raise TimeoutError(\"El indexer no completó dentro del tiempo esperado\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Función auxiliar para generar embeddings de queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_query_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Generate embedding for a query using Azure OpenAI.\n",
    "    \"\"\"\n",
    "    response = embeddings_client.embeddings.create(\n",
    "        model=EMBEDDING_DEPLOYMENT,\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "print(\"Función de embedding lista\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Búsqueda Híbrida (Texto + Vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query: str, top_k: int = 3) -> None:\n",
    "    print(f\"\\nBúsqueda híbrida (texto + vector): '{query}'\")\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_vector = generate_query_embedding(query)\n",
    "    \n",
    "    # Create vectorized query\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=query_vector,\n",
    "        k_nearest_neighbors=top_k,\n",
    "        fields=\"embedding\"\n",
    "    )\n",
    "    \n",
    "    # Execute hybrid search (text + vector)\n",
    "    results = search_client.search(\n",
    "        search_text=query,  # Text search also\n",
    "        vector_queries=[vector_query],\n",
    "        select=[\"id\", \"content\", \"category\", \"tags\"],\n",
    "        top=top_k\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nResultados:\")\n",
    "    found = False\n",
    "    for i, result in enumerate(results, 1):\n",
    "        found = True\n",
    "        snippet = result[\"content\"].replace(\"\\n\", \" \")\n",
    "        score = result.get(\"@search.score\", 0.0)\n",
    "        category = result.get(\"category\", \"N/A\")\n",
    "        tags = result.get(\"tags\", [])\n",
    "        \n",
    "        print(f\"\\n{i}. Score: {score:.4f}\")\n",
    "        print(f\"   ID: {result['id']}\")\n",
    "        print(f\"   Categoría: {category}\")\n",
    "        print(f\"   Tags: {', '.join(tags) if tags else 'N/A'}\")\n",
    "        print(f\"   Contenido: {snippet}...\")\n",
    "    \n",
    "    if not found:\n",
    "        print(\"  (No se encontraron resultados)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Búsqueda con Filtros (por categoría y tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_search(query: str, category: str = None, tags: List[str] = None, top_k: int = 3) -> None:\n",
    "    filters = []\n",
    "    if category:\n",
    "        filters.append(f\"category eq '{category}'\")\n",
    "    if tags:\n",
    "        tag_filters = [f\"tags/any(t: t eq '{tag}')\" for tag in tags]\n",
    "        filters.extend(tag_filters)\n",
    "    \n",
    "    filter_str = \" and \".join(filters) if filters else None\n",
    "    \n",
    "    print(f\"\\nBúsqueda con filtros: '{query}'\")\n",
    "    if filter_str:\n",
    "        print(f\"   Filtros: {filter_str}\")\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_vector = generate_query_embedding(query)\n",
    "    \n",
    "    # Create vectorized query\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=query_vector,\n",
    "        k_nearest_neighbors=50,  # Search more candidates before filtering\n",
    "        fields=\"embedding\"\n",
    "    )\n",
    "    \n",
    "    # Execute search with filters\n",
    "    results = search_client.search(\n",
    "        search_text=query,\n",
    "        vector_queries=[vector_query],\n",
    "        filter=filter_str,\n",
    "        select=[\"id\", \"content\", \"category\", \"tags\"],\n",
    "        top=top_k\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nResultados:\")\n",
    "    found = False\n",
    "    for i, result in enumerate(results, 1):\n",
    "        found = True\n",
    "        snippet = result[\"content\"].replace(\"\\n\", \" \")\n",
    "        score = result.get(\"@search.score\", 0.0)\n",
    "        category = result.get(\"category\", \"N/A\")\n",
    "        tags = result.get(\"tags\", [])\n",
    "        \n",
    "        print(f\"\\n{i}. Score: {score:.4f}\")\n",
    "        print(f\"   ID: {result['id']}\")\n",
    "        print(f\"   Categoría: {category}\")\n",
    "        print(f\"   Tags: {', '.join(tags) if tags else 'N/A'}\")\n",
    "        print(f\"   Contenido: {snippet}...\")\n",
    "    \n",
    "    if not found:\n",
    "        print(\"  (No se encontraron resultados)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Ejemplos de búsqueda con filtros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_search(\"comer\", category=\"Bilbao\")\n",
    "\n",
    "filtered_search(\"museos\", tags=[\"museos\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Realizar búsquedas híbridas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_search(\"Guggenheim\")\n",
    "\n",
    "hybrid_search(\"deportes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
