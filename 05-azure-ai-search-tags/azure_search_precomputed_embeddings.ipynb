{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Azure AI Search con Embeddings Pre-calculados\n",
        "\n",
        "Este notebook utiliza los datos generados en el m√≥dulo anterior (04-document-ingestion) que ya incluyen:\n",
        "- Embeddings vectoriales\n",
        "- Categor√≠as\n",
        "- Tags\n",
        "\n",
        "A diferencia del enfoque con Skillset, aqu√≠ NO necesitamos generar embeddings durante la ingesta,\n",
        "simplemente indexamos los vectores que ya existen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "import pathlib\n",
        "import time\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.core.exceptions import ResourceExistsError\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "\n",
        "from azure.search.documents import SearchClient\n",
        "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
        "from azure.search.documents.models import VectorizedQuery\n",
        "from azure.search.documents.indexes.models import (\n",
        "    HnswAlgorithmConfiguration,\n",
        "    IndexingParameters,\n",
        "    SearchField,\n",
        "    SearchFieldDataType,\n",
        "    SearchIndex,\n",
        "    SearchIndexer,\n",
        "    SearchIndexerDataContainer,\n",
        "    SearchIndexerDataSourceConnection,\n",
        "    SimpleField,\n",
        "    SearchableField,\n",
        "    VectorSearch,\n",
        "    VectorSearchProfile,\n",
        ")\n",
        "from dotenv import load_dotenv\n",
        "import openai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuraci√≥n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv(override=True)\n",
        "\n",
        "# Azure Search\n",
        "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
        "AZURE_SEARCH_KEY = os.getenv(\"AZURE_SEARCH_KEY\")\n",
        "\n",
        "# Azure Storage\n",
        "AZURE_STORAGE_CONNECTION_STRING = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
        "BLOB_CONTAINER_NAME = os.getenv(\"AZURE_STORAGE_CONTAINER\", \"documents-precomputed-vectors\")\n",
        "BLOB_PREFIX = \"json-chunks\"\n",
        "\n",
        "# Nombres de recursos\n",
        "INDEX_NAME = \"chunks-precomputed-vectors-index\"\n",
        "DATA_SOURCE_NAME = \"chunks-blob-datasource\"\n",
        "INDEXER_NAME = \"chunks-blob-indexer\"\n",
        "VECTOR_PROFILE_NAME = \"chunksProfile\"\n",
        "ALGORITHM_NAME = \"chunksHnsw\"\n",
        "\n",
        "# Archivo de datos del m√≥dulo anterior\n",
        "DATA_FILE = pathlib.Path(\"..\") / \"04-document-ingestion\" / \"data\" / \"rag_ingested_chunks.json\"\n",
        "\n",
        "print(\"‚úì Configuraci√≥n cargada\")\n",
        "print(f\"  - Archivo de datos: {DATA_FILE}\")\n",
        "print(f\"  - √çndice: {INDEX_NAME}\")\n",
        "print(f\"  - Contenedor: {BLOB_CONTAINER_NAME}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Inicializar Clientes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validar variables de entorno\n",
        "required_vars = [\n",
        "    \"AZURE_SEARCH_ENDPOINT\",\n",
        "    \"AZURE_SEARCH_KEY\",\n",
        "    \"AZURE_STORAGE_CONNECTION_STRING\",\n",
        "    \"GITHUB_TOKEN\",\n",
        "]\n",
        "\n",
        "missing = [var for var in required_vars if os.getenv(var) is None]\n",
        "if missing:\n",
        "    raise EnvironmentError(f\"Faltan variables de entorno: {', '.join(missing)}\")\n",
        "\n",
        "# Inicializar clientes de Azure Search\n",
        "credential = AzureKeyCredential(AZURE_SEARCH_KEY)\n",
        "index_client = SearchIndexClient(endpoint=AZURE_SEARCH_ENDPOINT, credential=credential)\n",
        "indexer_client = SearchIndexerClient(endpoint=AZURE_SEARCH_ENDPOINT, credential=credential)\n",
        "search_client = SearchClient(endpoint=AZURE_SEARCH_ENDPOINT, index_name=INDEX_NAME, credential=credential)\n",
        "\n",
        "# Cliente de OpenAI (GitHub Models) para generar embeddings en las queries\n",
        "openai_client = openai.OpenAI(\n",
        "    base_url=\"https://models.github.ai/inference\",\n",
        "    api_key=os.environ.get(\"GITHUB_TOKEN\")\n",
        ")\n",
        "\n",
        "print(\"‚úì Clientes inicializados\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Cargar Documentos con Embeddings Pre-calculados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not DATA_FILE.exists():\n",
        "    raise FileNotFoundError(f\"No se encuentra el archivo: {DATA_FILE}\")\n",
        "\n",
        "with open(DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    documents = json.load(f)\n",
        "\n",
        "print(f\"‚úì Cargados {len(documents)} documentos\")\n",
        "print(f\"\\nEjemplo del primer documento:\")\n",
        "sample_doc = documents[0].copy()\n",
        "# Truncar embedding para visualizaci√≥n\n",
        "if 'embedding' in sample_doc:\n",
        "    embedding_len = len(sample_doc['embedding'])\n",
        "    sample_doc['embedding'] = f\"[vector de {embedding_len} dimensiones]\"\n",
        "print(json.dumps(sample_doc, indent=2, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Subir Documentos a Blob Storage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüöö Subiendo documentos al contenedor de Blob Storage...\")\n",
        "blob_service = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)\n",
        "container_client = blob_service.get_container_client(BLOB_CONTAINER_NAME)\n",
        "\n",
        "try:\n",
        "    container_client.create_container()\n",
        "    print(f\"  ‚úì Contenedor '{BLOB_CONTAINER_NAME}' creado\")\n",
        "except ResourceExistsError:\n",
        "    print(f\"  ‚Ä¢ Contenedor '{BLOB_CONTAINER_NAME}' ya existe, se reutiliza\")\n",
        "\n",
        "uploaded_count = 0\n",
        "for doc in documents:\n",
        "    blob_name = f\"{BLOB_PREFIX}/{doc['id']}.json\"\n",
        "    data = json.dumps(doc, ensure_ascii=False).encode(\"utf-8\")\n",
        "    container_client.upload_blob(name=blob_name, data=data, overwrite=True)\n",
        "    uploaded_count += 1\n",
        "    if uploaded_count % 100 == 0:\n",
        "        print(f\"  ‚Ä¢ Subidos {uploaded_count}/{len(documents)} documentos...\")\n",
        "\n",
        "print(f\"  ‚úì {uploaded_count} documentos subidos correctamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Crear √çndice de Azure AI Search\n",
        "\n",
        "El √≠ndice acepta vectores pre-calculados. No necesitamos vectorizer para la ingesta.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüß± Creando √≠ndice para vectores pre-calculados...\")\n",
        "\n",
        "# Definir campos del √≠ndice\n",
        "fields = [\n",
        "    SimpleField(\n",
        "        name=\"id\", \n",
        "        type=SearchFieldDataType.String, \n",
        "        key=True, \n",
        "        filterable=True, \n",
        "        sortable=True\n",
        "    ),\n",
        "    SearchableField(\n",
        "        name=\"content\", \n",
        "        type=SearchFieldDataType.String, \n",
        "        searchable=True\n",
        "    ),\n",
        "    SimpleField(\n",
        "        name=\"category\", \n",
        "        type=SearchFieldDataType.String, \n",
        "        filterable=True, \n",
        "        facetable=True, \n",
        "        sortable=True\n",
        "    ),\n",
        "    SimpleField(\n",
        "        name=\"source\", \n",
        "        type=SearchFieldDataType.String, \n",
        "        filterable=True, \n",
        "        facetable=True\n",
        "    ),\n",
        "    SimpleField(\n",
        "        name=\"tags\",\n",
        "        type=SearchFieldDataType.Collection(SearchFieldDataType.String),\n",
        "        filterable=True,\n",
        "        facetable=True,\n",
        "    ),\n",
        "    SearchField(\n",
        "        name=\"embedding\",\n",
        "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
        "        searchable=True,\n",
        "        vector_search_dimensions=1536,\n",
        "        vector_search_profile_name=VECTOR_PROFILE_NAME,\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Configurar b√∫squeda vectorial\n",
        "vector_search = VectorSearch(\n",
        "    profiles=[\n",
        "        VectorSearchProfile(\n",
        "            name=VECTOR_PROFILE_NAME,\n",
        "            algorithm_configuration_name=ALGORITHM_NAME,\n",
        "        )\n",
        "    ],\n",
        "    algorithms=[HnswAlgorithmConfiguration(name=ALGORITHM_NAME)],\n",
        ")\n",
        "\n",
        "index = SearchIndex(name=INDEX_NAME, fields=fields, vector_search=vector_search)\n",
        "\n",
        "# Eliminar √≠ndice anterior si existe\n",
        "try:\n",
        "    index_client.delete_index(INDEX_NAME)\n",
        "    print(\"  ‚Ä¢ √çndice anterior eliminado\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "index_client.create_index(index)\n",
        "print(f\"  ‚úì √çndice '{INDEX_NAME}' creado\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Crear Data Source\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüîå Creando data source del contenedor...\")\n",
        "\n",
        "# Limpieza preventiva del indexer\n",
        "try:\n",
        "    indexer_client.delete_indexer(INDEXER_NAME)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "container = SearchIndexerDataContainer(name=BLOB_CONTAINER_NAME, query=f\"{BLOB_PREFIX}\")\n",
        "data_source = SearchIndexerDataSourceConnection(\n",
        "    name=DATA_SOURCE_NAME,\n",
        "    type=\"azureblob\",\n",
        "    connection_string=AZURE_STORAGE_CONNECTION_STRING,\n",
        "    container=container,\n",
        "    description=\"Blob container con chunks y embeddings pre-calculados\",\n",
        ")\n",
        "\n",
        "try:\n",
        "    indexer_client.delete_data_source_connection(DATA_SOURCE_NAME)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "indexer_client.create_data_source_connection(data_source)\n",
        "print(f\"  ‚úì Data source '{DATA_SOURCE_NAME}' creada\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Crear Indexer (SIN Skillset)\n",
        "\n",
        "Como los embeddings ya est√°n calculados, NO necesitamos un skillset.\n",
        "El indexer simplemente lee los campos del JSON y los mapea al √≠ndice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n‚öôÔ∏è  Creando indexer (sin skillset, vectores pre-calculados)...\")\n",
        "\n",
        "indexer = SearchIndexer(\n",
        "    name=INDEXER_NAME,\n",
        "    data_source_name=DATA_SOURCE_NAME,\n",
        "    target_index_name=INDEX_NAME,\n",
        "    description=\"Indexer para chunks con embeddings pre-calculados\",\n",
        "    parameters=IndexingParameters(configuration={\"parsingMode\": \"json\"}),\n",
        ")\n",
        "\n",
        "try:\n",
        "    indexer_client.delete_indexer(INDEXER_NAME)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "indexer_client.create_indexer(indexer)\n",
        "print(f\"  ‚úì Indexer '{INDEXER_NAME}' creado\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Ejecutar Indexer y Esperar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n‚ñ∂Ô∏è  Ejecutando indexer y esperando a que finalice...\")\n",
        "indexer_client.run_indexer(INDEXER_NAME)\n",
        "\n",
        "timeout_seconds = 300\n",
        "waited = 0\n",
        "poll_interval = 5\n",
        "\n",
        "while waited < timeout_seconds:\n",
        "    status = indexer_client.get_indexer_status(INDEXER_NAME)\n",
        "    last_result = status.last_result\n",
        "    \n",
        "    if last_result and last_result.status == \"success\":\n",
        "        print(f\"  ‚úì Indexer completado exitosamente\")\n",
        "        print(f\"    - Documentos procesados: {last_result.item_count}\")\n",
        "        print(f\"    - Documentos fallidos: {last_result.failed_item_count}\")\n",
        "        if last_result.failed_item_count > 0:\n",
        "            print(f\"    ‚ö†Ô∏è  Errores: {last_result.error_message}\")\n",
        "        break\n",
        "    elif last_result and last_result.status == \"transientFailure\":\n",
        "        print(f\"  ‚ö†Ô∏è Fallo transitorio: {last_result.error_message}\")\n",
        "    elif last_result and last_result.status == \"error\":\n",
        "        raise RuntimeError(f\"Indexer fall√≥: {last_result.error_message}\")\n",
        "    \n",
        "    print(f\"  ‚Ä¢ Estado: {status.status} (esperados {waited}/{timeout_seconds}s)\")\n",
        "    time.sleep(poll_interval)\n",
        "    waited += poll_interval\n",
        "else:\n",
        "    raise TimeoutError(\"El indexer no complet√≥ dentro del tiempo esperado\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Funci√≥n auxiliar para generar embeddings de queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_query_embedding(text: str) -> List[float]:\n",
        "    \"\"\"\n",
        "    Genera el embedding de una query usando text-embedding-3-small.\n",
        "    \"\"\"\n",
        "    response = openai_client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        input=text\n",
        "    )\n",
        "    return response.data[0].embedding\n",
        "\n",
        "print(\"‚úì Funci√≥n de embedding lista\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. B√∫squeda Vectorial Pura\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vector_search(query: str, top_k: int = 3) -> None:\n",
        "    print(f\"\\nüîé B√∫squeda vectorial: '{query}'\")\n",
        "    \n",
        "    # Generar embedding de la query\n",
        "    query_vector = generate_query_embedding(query)\n",
        "    \n",
        "    # Crear consulta vectorizada\n",
        "    vector_query = VectorizedQuery(\n",
        "        vector=query_vector,\n",
        "        k_nearest_neighbors=top_k,\n",
        "        fields=\"embedding\"\n",
        "    )\n",
        "    \n",
        "    # Ejecutar b√∫squeda\n",
        "    results = search_client.search(\n",
        "        search_text=None,\n",
        "        vector_queries=[vector_query],\n",
        "        select=[\"id\", \"content\", \"category\", \"tags\"],\n",
        "        top=top_k\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nResultados:\")\n",
        "    found = False\n",
        "    for i, result in enumerate(results, 1):\n",
        "        found = True\n",
        "        snippet = result[\"content\"][:150].replace(\"\\n\", \" \")\n",
        "        score = result.get(\"@search.score\", 0.0)\n",
        "        category = result.get(\"category\", \"N/A\")\n",
        "        tags = result.get(\"tags\", [])\n",
        "        \n",
        "        print(f\"\\n{i}. Score: {score:.4f}\")\n",
        "        print(f\"   ID: {result['id']}\")\n",
        "        print(f\"   Categor√≠a: {category}\")\n",
        "        print(f\"   Tags: {', '.join(tags) if tags else 'N/A'}\")\n",
        "        print(f\"   Contenido: {snippet}...\")\n",
        "    \n",
        "    if not found:\n",
        "        print(\"  (No se encontraron resultados)\")\n",
        "\n",
        "# Realizar b√∫squedas de ejemplo\n",
        "vector_search(\"¬øQu√© museos puedo visitar en Bilbao?\")\n",
        "vector_search(\"Gastronom√≠a y pintxos\")\n",
        "vector_search(\"C√≥mo llegar y transporte\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. B√∫squeda H√≠brida (Texto + Vector)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hybrid_search(query: str, top_k: int = 3) -> None:\n",
        "    print(f\"\\nüîé B√∫squeda h√≠brida (texto + vector): '{query}'\")\n",
        "    \n",
        "    # Generar embedding de la query\n",
        "    query_vector = generate_query_embedding(query)\n",
        "    \n",
        "    # Crear consulta vectorizada\n",
        "    vector_query = VectorizedQuery(\n",
        "        vector=query_vector,\n",
        "        k_nearest_neighbors=top_k,\n",
        "        fields=\"embedding\"\n",
        "    )\n",
        "    \n",
        "    # Ejecutar b√∫squeda h√≠brida (texto + vector)\n",
        "    results = search_client.search(\n",
        "        search_text=query,  # B√∫squeda de texto tambi√©n\n",
        "        vector_queries=[vector_query],\n",
        "        select=[\"id\", \"content\", \"category\", \"tags\"],\n",
        "        top=top_k\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nResultados:\")\n",
        "    found = False\n",
        "    for i, result in enumerate(results, 1):\n",
        "        found = True\n",
        "        snippet = result[\"content\"][:150].replace(\"\\n\", \" \")\n",
        "        score = result.get(\"@search.score\", 0.0)\n",
        "        category = result.get(\"category\", \"N/A\")\n",
        "        tags = result.get(\"tags\", [])\n",
        "        \n",
        "        print(f\"\\n{i}. Score: {score:.4f}\")\n",
        "        print(f\"   ID: {result['id']}\")\n",
        "        print(f\"   Categor√≠a: {category}\")\n",
        "        print(f\"   Tags: {', '.join(tags) if tags else 'N/A'}\")\n",
        "        print(f\"   Contenido: {snippet}...\")\n",
        "    \n",
        "    if not found:\n",
        "        print(\"  (No se encontraron resultados)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. B√∫squeda con Filtros (por categor√≠a y tags)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filtered_search(query: str, category: str = None, tags: List[str] = None, top_k: int = 3) -> None:\n",
        "    filters = []\n",
        "    if category:\n",
        "        filters.append(f\"category eq '{category}'\")\n",
        "    if tags:\n",
        "        tag_filters = [f\"tags/any(t: t eq '{tag}')\" for tag in tags]\n",
        "        filters.extend(tag_filters)\n",
        "    \n",
        "    filter_str = \" and \".join(filters) if filters else None\n",
        "    \n",
        "    print(f\"\\nüîé B√∫squeda con filtros: '{query}'\")\n",
        "    if filter_str:\n",
        "        print(f\"   Filtros: {filter_str}\")\n",
        "    \n",
        "    # Generar embedding de la query\n",
        "    query_vector = generate_query_embedding(query)\n",
        "    \n",
        "    # Crear consulta vectorizada\n",
        "    vector_query = VectorizedQuery(\n",
        "        vector=query_vector,\n",
        "        k_nearest_neighbors=50,  # Buscar m√°s candidatos antes de filtrar\n",
        "        fields=\"embedding\"\n",
        "    )\n",
        "    \n",
        "    # Ejecutar b√∫squeda con filtros\n",
        "    results = search_client.search(\n",
        "        search_text=query,\n",
        "        vector_queries=[vector_query],\n",
        "        filter=filter_str,\n",
        "        select=[\"id\", \"content\", \"category\", \"tags\"],\n",
        "        top=top_k\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nResultados:\")\n",
        "    found = False\n",
        "    for i, result in enumerate(results, 1):\n",
        "        found = True\n",
        "        snippet = result[\"content\"][:150].replace(\"\\n\", \" \")\n",
        "        score = result.get(\"@search.score\", 0.0)\n",
        "        category = result.get(\"category\", \"N/A\")\n",
        "        tags = result.get(\"tags\", [])\n",
        "        \n",
        "        print(f\"\\n{i}. Score: {score:.4f}\")\n",
        "        print(f\"   ID: {result['id']}\")\n",
        "        print(f\"   Categor√≠a: {category}\")\n",
        "        print(f\"   Tags: {', '.join(tags) if tags else 'N/A'}\")\n",
        "        print(f\"   Contenido: {snippet}...\")\n",
        "    \n",
        "    if not found:\n",
        "        print(\"  (No se encontraron resultados)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Ejemplos de b√∫squeda con filtros\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered_search(\"informaci√≥n tur√≠stica\", category=\"Bilbao\")\n",
        "\n",
        "filtered_search(\"museos\", tags=[\"museos\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Realizar b√∫squedas h√≠bridas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hybrid_search(\"Guggenheim\")\n",
        "\n",
        "hybrid_search(\"running y deportes\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
