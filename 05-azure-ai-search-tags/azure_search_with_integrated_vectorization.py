"""
Ejercicio: Azure AI Search con Integrated Vectorization (Vectorizaci√≥n Integrada)

Este ejercicio demuestra c√≥mo usar Azure AI Search con vectorizaci√≥n integrada:
1. Azure genera autom√°ticamente los embeddings (no los calculas en local)
2. Solo subes el texto plano de los documentos
3. Azure usa Azure OpenAI para crear los embeddings de forma consistente
4. Mantiene la consistencia entre indexaci√≥n y b√∫squeda

VENTAJAS vs. Embeddings Locales:
‚úÖ No necesitas generar embeddings localmente
‚úÖ Azure sabe qu√© modelo usa (lo configuras en el √≠ndice)
‚úÖ Consistencia garantizada entre indexaci√≥n y b√∫squeda
‚úÖ Actualizaciones autom√°ticas cuando cambias el modelo
‚úÖ Menos c√≥digo, menos errores

Requisitos:
- Azure AI Search service
- Azure OpenAI service con un deployment de embeddings
- Variables de entorno en .env:
  - AZURE_SEARCH_ENDPOINT
  - AZURE_SEARCH_KEY
  - AZURE_OPENAI_ENDPOINT
  - AZURE_OPENAI_KEY
  - AZURE_OPENAI_EMBEDDING_DEPLOYMENT (ej: "text-embedding-3-small")
"""

import json
import os
import pathlib
import time
from typing import List, Dict, Any

from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.models import VectorizableTextQuery
from azure.search.documents.indexes.models import (
    SearchIndex,
    SearchField,
    SearchFieldDataType,
    SimpleField,
    SearchableField,
    VectorSearch,
    VectorSearchProfile,
    HnswAlgorithmConfiguration,
    AzureOpenAIVectorizer,
    AzureOpenAIVectorizerParameters,
    AzureOpenAIModelName,
)
from dotenv import load_dotenv

# Cargar variables de entorno
load_dotenv(override=True)

# Configuraci√≥n Azure AI Search
AZURE_SEARCH_ENDPOINT = os.getenv("AZURE_SEARCH_ENDPOINT")
AZURE_SEARCH_KEY = os.getenv("AZURE_SEARCH_KEY")
INDEX_NAME = "documents-integrated-vectorization"

# Configuraci√≥n Azure OpenAI (para que Azure genere los embeddings)
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_KEY = os.getenv("AZURE_OPENAI_KEY")
AZURE_OPENAI_EMBEDDING_DEPLOYMENT = os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT")

# Clientes de Azure AI Search
credential = AzureKeyCredential(AZURE_SEARCH_KEY)
index_client = SearchIndexClient(endpoint=AZURE_SEARCH_ENDPOINT, credential=credential)
search_client = SearchClient(endpoint=AZURE_SEARCH_ENDPOINT, index_name=INDEX_NAME, credential=credential)


def create_search_index_with_vectorization():
    """
    Crea un √≠ndice con VECTORIZACI√ìN INTEGRADA.
    
    La diferencia clave: configuramos un AzureOpenAIVectorizer que le dice a Azure
    qu√© modelo de Azure OpenAI usar para generar embeddings autom√°ticamente.
    """
    print(f"\nüîß Creando √≠ndice con vectorizaci√≥n integrada '{INDEX_NAME}'...")

    # Configurar el vectorizer (le dice a Azure c√≥mo generar embeddings)
    vectorizer = AzureOpenAIVectorizer(
        vectorizer_name="myVectorizer",
        parameters=AzureOpenAIVectorizerParameters(
            resource_url=AZURE_OPENAI_ENDPOINT,
            deployment_name=AZURE_OPENAI_EMBEDDING_DEPLOYMENT,
            api_key=AZURE_OPENAI_KEY,
            model_name="text-embedding-ada-002",
        ),
    )
    print(f"  ‚úì Vectorizer configurado:")
    print(vectorizer)

    # Definir los campos del √≠ndice
    fields = [
        SimpleField(
            name="id",
            type=SearchFieldDataType.String,
            key=True,
            sortable=True,
            filterable=True,
        ),
        SearchableField(
            name="content",
            type=SearchFieldDataType.String,
            searchable=True,
        ),
        SimpleField(
            name="category",
            type=SearchFieldDataType.String,
            filterable=True,
            facetable=True,
        ),
        SimpleField(
            name="source",
            type=SearchFieldDataType.String,
            filterable=True,
            facetable=True,
        ),
        SimpleField(
            name="tags",
            type=SearchFieldDataType.Collection(SearchFieldDataType.String),
            filterable=True,
            facetable=True,
        ),
        # Campo vectorial: Azure lo genera autom√°ticamente del campo 'content'
        SearchField(
            name="contentVector",
            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
            searchable=True,
            vector_search_dimensions=1536,  # Dimensiones de text-embedding-3-small
            vector_search_profile_name="myProfile",
        ),
    ]

    # Configurar b√∫squeda vectorial con el vectorizer
    vector_search = VectorSearch(
        profiles=[
            VectorSearchProfile(
                name="myProfile",
                algorithm_configuration_name="myAlgorithm",
                vectorizer_name="myVectorizer",  # üëà Asocia el vectorizer al profile
            )
        ],
        algorithms=[
            HnswAlgorithmConfiguration(name="myAlgorithm")
        ],
        vectorizers=[vectorizer],  # üëà Agrega el vectorizer al √≠ndice
    )

    # Crear el √≠ndice
    index = SearchIndex(
        name=INDEX_NAME,
        fields=fields,
        vector_search=vector_search,
    )

    # Eliminar √≠ndice si ya existe
    try:
        index_client.delete_index(INDEX_NAME)
        print(f"  ‚úì √çndice anterior eliminado")
    except Exception:
        pass

    # Crear nuevo √≠ndice
    index_client.create_index(index)
    print(f"  ‚úì √çndice '{INDEX_NAME}' creado con vectorizaci√≥n integrada")
    print(f"  ‚úì Azure generar√° embeddings usando: {AZURE_OPENAI_EMBEDDING_DEPLOYMENT}")
    print(f"  ‚úì Campos: id, content, category, source, tags, contentVector")


def index_documents():
    """
    Indexa documentos SIN generar embeddings localmente.
    
    IMPORTANTE: Solo subimos el texto plano. Azure genera los embeddings autom√°ticamente.
    """
    print(f"\nüìù Indexando documentos (Azure generar√° los embeddings)...")

    # Cargar documentos
    documents = json.load(
        open(pathlib.Path(__file__).parent / "data/documents_with_metadata.json", "r", encoding="utf-8")
    )

    # Preparar documentos para indexar
    docs_to_index = []
    for doc in documents:
        print(f"  Preparando: {doc['id']} - Categor√≠a: {doc['category']}")

        # ‚ö†Ô∏è NOTA: NO generamos embeddings aqu√≠ - Azure lo hace autom√°ticamente
        doc_to_index = {
            "id": doc["id"],
            "content": doc["content"],  # üëà Solo el texto
            "category": doc["category"],
            "source": doc["source"],
            "tags": doc["tags"],
            # contentVector NO se incluye - Azure lo genera autom√°ticamente
        }
        docs_to_index.append(doc_to_index)

    # Subir documentos (Azure generar√° los embeddings)
    result = search_client.upload_documents(documents=docs_to_index)
    print(f"  ‚úì {len(docs_to_index)} documentos subidos")
    print(f"  ‚úì Azure est√° generando los embeddings en background...")
    
    # Esperar a que los documentos se indexen y vectoricen completamente
    print(f"  ‚è≥ Esperando a que la vectorizaci√≥n complete...")
    time.sleep(10)  # M√°s tiempo porque Azure necesita generar los embeddings

    return documents


def search_by_tag(category: str = None, source: str = None, tags: List[str] = None):
    """
    B√∫squeda filtrada por tags usando filtros OData
    """
    filters = []

    if category:
        filters.append(f"category eq '{category}'")

    if source:
        filters.append(f"source eq '{source}'")

    if tags:
        tag_filters = [f"tags/any(t: t eq '{tag}')" for tag in tags]
        filters.append(f"({' or '.join(tag_filters)})")

    filter_expression = " and ".join(filters) if filters else None

    print(f"\nüîç B√∫squeda con filtros:")
    if category:
        print(f"  - Categor√≠a: {category}")
    if source:
        print(f"  - Fuente: {source}")
    if tags:
        print(f"  - Tags: {tags}")

    if filter_expression:
        print(f"  Expresi√≥n de filtro: {filter_expression}")

    # Realizar b√∫squeda
    results = search_client.search(
        search_text="*",
        filter=filter_expression,
        select=["id", "content", "category", "source", "tags"],
        top=3
    )

    print(f"\nüìã Resultados:")
    count = 0
    for result in results:
        count += 1
        print(f"\n  {count}. ID: {result['id']}")
        print(f"     Categor√≠a: {result['category']}")
        print(f"     Fuente: {result['source']}")
        print(f"     Tags: {', '.join(result['tags'])}")
        print(f"     Contenido: {result['content'][:100]}...")

    if count == 0:
        print("  ‚ö†Ô∏è  No se encontraron resultados")

    return count


def hybrid_search(query: str, category: str = None, tags: List[str] = None):
    """
    B√∫squeda h√≠brida con vectorizaci√≥n integrada.
    
    IMPORTANTE: Usamos VectorizableTextQuery en lugar de VectorizedQuery.
    Esto le dice a Azure: "toma este texto, genera el embedding usando el modelo
    que configuramos en el √≠ndice, y b√∫scalo".
    """
    
    # Construir filtros
    filters = []
    if category:
        filters.append(f"category eq '{category}'")
    if tags:
        tag_filters = [f"tags/any(t: t eq '{tag}')" for tag in tags]
        filters.append(f"({' or '.join(tag_filters)})")

    filter_expression = " and ".join(filters) if filters else None

    print(f"\nüîç B√∫squeda h√≠brida (con vectorizaci√≥n integrada):")
    print(f"  - Query: '{query}'")
    if category:
        print(f"  - Categor√≠a: {category}")
    if tags:
        print(f"  - Tags: {tags}")

    # üëà CLAVE: VectorizableTextQuery - Azure genera el embedding autom√°ticamente
    vector_query = VectorizableTextQuery(
        text=query,  # Solo pasamos el texto
        k_nearest_neighbors=5,
        fields="contentVector"
    )

    # Realizar b√∫squeda h√≠brida
    results = search_client.search(
        search_text=query,
        vector_queries=[vector_query],  # Azure vectoriza el query autom√°ticamente
        filter=filter_expression,
        select=["id", "content", "category", "source", "tags"],
        top=5
    )

    print(f"\nüìã Resultados (ordenados por relevancia):")
    count = 0
    for result in results:
        count += 1
        score = result.get('@search.score', 0)
        print(f"\n  {count}. ID: {result['id']} (Score: {score:.4f})")
        print(f"     Categor√≠a: {result['category']}")
        print(f"     Tags: {', '.join(result['tags'])}")
        print(f"     Contenido: {result['content'][:150]}...")

    if count == 0:
        print("  ‚ö†Ô∏è  No se encontraron resultados")

    return count


def vector_only_search(query: str):
    """
    B√∫squeda PURAMENTE vectorial.
    Ignora coincidencias de palabras clave (search_text=None).
    Sirve para demostrar que la b√∫squeda entiende significados, no solo palabras.
    """
    print(f"\nüß† B√∫squeda SOLO Vectorial (Sem√°ntica pura):")
    print(f"  - Query: '{query}'")

    vector_query = VectorizableTextQuery(
        text=query,
        k_nearest_neighbors=5,
        fields="contentVector"
    )

    # search_text=None desactiva la b√∫squeda por palabras clave (BM25)
    results = search_client.search(
        search_text=None,  
        vector_queries=[vector_query],
        select=["id", "content", "category"],
        top=3
    )

    print(f"\nüìã Resultados sem√°nticos:")
    count = 0
    for result in results:
        count += 1
        score = result.get('@search.score', 0)
        print(f"\n  {count}. ID: {result['id']} (Score: {score:.4f})")
        print(f"     Contenido: {result['content'][:100]}...")

    if count == 0:
        print("  ‚ö†Ô∏è  No se encontraron resultados")

def main():
    """Funci√≥n principal"""

    print("=" * 70)
    print("  Azure AI Search - Vectorizaci√≥n Integrada con Tags")
    print("=" * 70)

    # 1. Crear √≠ndice con vectorizaci√≥n integrada
    #create_search_index_with_vectorization()

    # 2. Indexar documentos (Azure genera los embeddings)
    #documents = index_documents()

    # 3. Ejemplo 1: Buscar por categor√≠a
    #search_by_tag(category="negocios")

    # 4. Ejemplo 2: Buscar documentos de salud
    #search_by_tag(category="salud")

    # 5. Ejemplo 3: Buscar con tag espec√≠fico
    #search_by_tag(tags=["ia"])

    # 6. Ejemplo 4: B√∫squeda h√≠brida sobre tecnolog√≠a
    #hybrid_search("lenguajes de programaci√≥n", category="tecnologia")

    # 7. Ejemplo 5: B√∫squeda h√≠brida sobre salud
    #hybrid_search("c√≥mo mejorar la salud del coraz√≥n", category="salud")

    # 8. Ejemplo 6: B√∫squeda sobre m√©tricas
    hybrid_search("jefe")

    vector_only_search("running") 

    # 9. Ejemplo 7: B√∫squeda con filtro de tags
    hybrid_search("b√∫squeda de informaci√≥n", tags=["rag"])

    print("\n" + "=" * 70)
    print("  ‚úÖ Ejercicio completado!")
    print("=" * 70)
    print("\nüí° Ventajas de la vectorizaci√≥n integrada:")
    print("  1. ‚úÖ No calculas embeddings localmente")
    print("  2. ‚úÖ Azure sabe qu√© modelo usa (configurado en el √≠ndice)")
    print("  3. ‚úÖ Consistencia autom√°tica entre indexaci√≥n y b√∫squeda")
    print("  4. ‚úÖ Menos c√≥digo, menos posibilidad de errores")
    print("  5. ‚úÖ Si cambias de modelo, solo actualizas la configuraci√≥n del √≠ndice")
    print("\nüîë Diferencias clave:")
    print("  - Embeddings locales: t√∫ generas, subes vectores, Azure solo almacena")
    print("  - Vectorizaci√≥n integrada: subes texto, Azure genera y almacena vectores")


if __name__ == "__main__":
    main()
