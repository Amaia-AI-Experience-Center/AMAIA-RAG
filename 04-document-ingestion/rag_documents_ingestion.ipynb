{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2e817a0",
   "metadata": {},
   "source": [
    "# Document Ingestion Pipeline\n",
    "## Ingesta de Documentos PDF con Embeddings y Tags Autom√°ticos\n",
    "\n",
    "Este notebook implementa un pipeline completo para:\n",
    "1. Extraer texto de archivos PDF\n",
    "2. Dividir en chunks procesables\n",
    "3. Generar embeddings con Azure OpenAI\n",
    "4. Clasificar autom√°ticamente con tags relevantes usando LLM\n",
    "5. Guardar metadatos enriquecidos en JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87863851",
   "metadata": {},
   "source": [
    "## 1. Configuraci√≥n de Variables de Entorno y Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656f1c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import openai\n",
    "import pymupdf4llm\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Inicializar cliente de OpenAI (GitHub Copilot Models)\n",
    "API_HOST = os.getenv(\"API_HOST\", \"github\")\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"https://models.github.ai/inference\", \n",
    "    api_key=os.environ.get(\"GITHUB_TOKEN\")\n",
    ")\n",
    "\n",
    "# Definir directorios\n",
    "data_dir = pathlib.Path.cwd() / \"data\"\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"‚úì Librer√≠as importadas correctamente\")\n",
    "print(f\"‚úì Variables de entorno cargadas\")\n",
    "print(f\"‚úì Directorio de datos: {data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29bc4c7",
   "metadata": {},
   "source": [
    "## 2. Cargar y Extraer Texto de Archivos PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97acdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar todos los archivos PDF en el directorio de datos\n",
    "filenames = [f.name for f in data_dir.glob(\"*.pdf\")]\n",
    "\n",
    "print(f\"\\nüìÑ Archivos PDF encontrados: {len(filenames)}\")\n",
    "for i, filename in enumerate(filenames, 1):\n",
    "    print(f\"   {i}. {filename}\")\n",
    "\n",
    "# Diccionario para almacenar el texto extra√≠do de cada archivo\n",
    "extracted_documents = {}\n",
    "\n",
    "for filename in filenames:\n",
    "    file_path = data_dir / filename\n",
    "    print(f\"\\nüîç Procesando: {filename}\")\n",
    "    \n",
    "    try:\n",
    "        # Extraer texto del PDF a formato markdown\n",
    "        md_text = pymupdf4llm.to_markdown(file_path)\n",
    "        extracted_documents[filename] = md_text\n",
    "        \n",
    "        print(f\"   ‚úì Texto extra√≠do: {len(md_text)} caracteres\")\n",
    "        print(f\"   Preview: {md_text[:150]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚úó Error al procesar: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bfbdd2",
   "metadata": {},
   "source": [
    "## 3. Dividir Texto en Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984661cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar el divisor de texto\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4o\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=125\n",
    ")\n",
    "\n",
    "# Almacenar todos los chunks\n",
    "all_chunks_raw = {}\n",
    "\n",
    "print(f\"\\n‚úÇÔ∏è  Dividiendo documentos en chunks...\")\n",
    "for filename, md_text in extracted_documents.items():\n",
    "    texts = text_splitter.create_documents([md_text])\n",
    "    all_chunks_raw[filename] = texts\n",
    "    \n",
    "    print(f\"   {filename}: {len(texts)} chunks\")\n",
    "    \n",
    "    # Mostrar primer chunk como ejemplo\n",
    "    if texts:\n",
    "        print(f\"      Ejemplo - Chunk 1: {texts[0].page_content[:100]}...\")\n",
    "\n",
    "print(f\"\\n‚úì Chunking completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a368e",
   "metadata": {},
   "source": [
    "## 4. Clasificar Chunks con LLM y Extraer Tags\n",
    "\n",
    "Se usar√° un modelo LLM para analizar autom√°ticamente cada chunk y extraer tags relevantes de la lista predefinida.\n",
    "\n",
    "**Tags disponibles:**\n",
    "- introduccion\n",
    "- barrios\n",
    "- gastronomia\n",
    "- museos\n",
    "- eventos\n",
    "- naturaleza\n",
    "- vida_nocturna\n",
    "- transporte\n",
    "- excursiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815544da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir tags disponibles\n",
    "AVAILABLE_TAGS = [\n",
    "    \"introduccion\",\n",
    "    \"barrios\",\n",
    "    \"gastronomia\",\n",
    "    \"museos\",\n",
    "    \"eventos\",\n",
    "    \"naturaleza\",\n",
    "    \"vida_nocturna\",\n",
    "    \"transporte\",\n",
    "    \"excursiones\"\n",
    "]\n",
    "\n",
    "def extract_tags_from_chunk(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Usa un LLM para extraer tags relevantes del contenido del chunk.\n",
    "    \n",
    "    Args:\n",
    "        text: Contenido del chunk\n",
    "    \n",
    "    Returns:\n",
    "        Lista de tags relevantes\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"Analiza el siguiente texto y A√ëADE SOLO SI SE MENCIONA EXPLICITAMENTE algo relacionado con uno o mas de estos tags: \n",
    "{', '.join(AVAILABLE_TAGS)}\n",
    "\n",
    "Texto: {text[:1000]}\n",
    "\n",
    "Responde SOLO con una lista de tags en formato JSON, ejemplo: [\"tag1\", \"tag2\"]\n",
    "Sin texto adicional, solo el JSON.\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "        # Parsear la respuesta JSON\n",
    "        response_text = response.choices[0].message.content.strip()\n",
    "        tags = json.loads(response_text)\n",
    "        \n",
    "        # Validar que todos los tags est√©n en la lista permitida\n",
    "        valid_tags = [tag for tag in tags if tag in AVAILABLE_TAGS]\n",
    "        return valid_tags if valid_tags else [\"introduccion\"]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Error al extraer tags: {e}\")\n",
    "        return [\"introduccion\"]\n",
    "\n",
    "print(\"‚úì Funci√≥n de extracci√≥n de tags definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbddccf",
   "metadata": {},
   "source": [
    "## 5. Generar Embeddings con Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789109ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Genera un embedding para un texto usando el modelo text-embedding-3-small.\n",
    "    \n",
    "    Args:\n",
    "        text: Texto para el cual generar embedding\n",
    "    \n",
    "    Returns:\n",
    "        Vector de embedding (lista de floats)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=text\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Error al generar embedding: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"‚úì Funci√≥n de generaci√≥n de embeddings definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb5b50",
   "metadata": {},
   "source": [
    "## 6. Estructurar Datos con Metadatos\n",
    "\n",
    "Crear la estructura completa con: id, content, category (nombre del archivo), source, tags, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58acf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = []\n",
    "chunk_counter = 1\n",
    "\n",
    "print(f\"\\nüì¶ Procesando chunks con metadatos...\")\n",
    "print(f\"‚ö†Ô∏è  Limitado a 3 chunks por PDF para conservar cuota de API\\n\")\n",
    "\n",
    "for filename, chunks in all_chunks_raw.items():\n",
    "    print(f\"   üìÑ {filename}\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        try:\n",
    "            text_content = chunk.page_content\n",
    "            \n",
    "            # Solo procesar tags y embeddings para los primeros 3 chunks\n",
    "            if i <= 3:\n",
    "                # Extraer tags usando LLM\n",
    "                print(f\"      ‚Ä¢ Chunk {i}/{len(chunks)}: extrayendo tags...\", end=\"\")\n",
    "                tags = extract_tags_from_chunk(text_content)\n",
    "                print(f\" ‚úì {tags}\")\n",
    "                \n",
    "                # Generar embedding\n",
    "                print(f\"      ‚Ä¢ Chunk {i}/{len(chunks)}: generando embedding...\", end=\"\")\n",
    "                embedding = generate_embedding(text_content)\n",
    "                print(f\" ‚úì\")\n",
    "            else:\n",
    "                # Para chunks > 3, usar valores por defecto\n",
    "                tags = [\"introduccion\"]\n",
    "                embedding = []\n",
    "                if i == 4:\n",
    "                    print(f\"      ‚Ä¢ Chunks restantes: usando valores por defecto (sin API calls)\")\n",
    "            \n",
    "            # Crear estructura de chunk con todos los metadatos\n",
    "            chunk_data = {\n",
    "                \"id\": f\"{filename.replace('.', '_')}-{i}\",\n",
    "                \"content\": text_content,\n",
    "                \"category\": filename.replace(\".pdf\", \"\"),  # Nombre del archivo sin extensi√≥n\n",
    "                \"source\": \"pdf_ingestion\",  # Fuente del documento\n",
    "                \"tags\": tags,\n",
    "                \"embedding\": embedding,\n",
    "                \"has_ai_processing\": i <= 3  # Indicar si se proces√≥ con IA\n",
    "            }\n",
    "            \n",
    "            all_chunks.append(chunk_data)\n",
    "            chunk_counter += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      ‚úó Error al procesar chunk {i}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úì Procesamiento completado: {len(all_chunks)} chunks generados\")\n",
    "print(f\"‚úì Chunks con IA (embeddings + tags): m√°x 3 por PDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2453db7a",
   "metadata": {},
   "source": [
    "## 7. Guardar Chunks Procesados en JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc330752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los chunks procesados en un archivo JSON\n",
    "output_file = data_dir / \"rag_ingested_chunks.json\"\n",
    "\n",
    "print(f\"\\nüíæ Guardando chunks en {output_file}...\")\n",
    "\n",
    "try:\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_chunks, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"‚úì Archivo guardado exitosamente\")\n",
    "    print(f\"‚úì Total de chunks: {len(all_chunks)}\")\n",
    "    print(f\"‚úì Tama√±o del archivo: {output_file.stat().st_size / 1024:.2f} KB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error al guardar archivo: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b236e",
   "metadata": {},
   "source": [
    "## Visualizaci√≥n de Resultados\n",
    "\n",
    "Revisar ejemplos de chunks procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Mostrar informaci√≥n resumen\n",
    "print(\"üìä Resumen de Chunks Procesados:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if all_chunks:\n",
    "    # Crear dataframe para an√°lisis\n",
    "    df_summary = pd.DataFrame([\n",
    "        {\n",
    "            \"ID\": chunk[\"id\"],\n",
    "            \"Categor√≠a\": chunk[\"category\"],\n",
    "            \"Tags\": \", \".join(chunk[\"tags\"]),\n",
    "            \"Longitud\": len(chunk[\"content\"]),\n",
    "            \"Has Embedding\": len(chunk[\"embedding\"]) > 0\n",
    "        }\n",
    "        for chunk in all_chunks[:10]  # Mostrar primeros 10\n",
    "    ])\n",
    "    \n",
    "    print(df_summary.to_string(index=False))\n",
    "    \n",
    "    if len(all_chunks) > 10:\n",
    "        print(f\"\\n... y {len(all_chunks) - 10} chunks m√°s\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"‚úì Total de chunks: {len(all_chunks)}\")\n",
    "    \n",
    "    # Estad√≠sticas por categor√≠a\n",
    "    categories = {}\n",
    "    for chunk in all_chunks:\n",
    "        cat = chunk[\"category\"]\n",
    "        categories[cat] = categories.get(cat, 0) + 1\n",
    "    \n",
    "    print(f\"\\nDistribuci√≥n por categor√≠a:\")\n",
    "    for cat, count in sorted(categories.items()):\n",
    "        print(f\"   ‚Ä¢ {cat}: {count} chunks\")\n",
    "    \n",
    "    # Estad√≠sticas de tags\n",
    "    all_tags = {}\n",
    "    for chunk in all_chunks:\n",
    "        for tag in chunk[\"tags\"]:\n",
    "            all_tags[tag] = all_tags.get(tag, 0) + 1\n",
    "    \n",
    "    print(f\"\\nDistribuci√≥n de tags:\")\n",
    "    for tag, count in sorted(all_tags.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   ‚Ä¢ {tag}: {count} veces\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No hay chunks para mostrar. Aseg√∫rate de que hay archivos PDF en el directorio /data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb15a7",
   "metadata": {},
   "source": [
    "## Ejemplo de Chunk Completo\n",
    "\n",
    "Visualizar la estructura detallada de un chunk procesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8436e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_chunks:\n",
    "    # Mostrar el primer chunk como ejemplo\n",
    "    example_chunk = all_chunks[0]\n",
    "    \n",
    "    print(\"üìå Ejemplo de Chunk Procesado:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ID: {example_chunk['id']}\")\n",
    "    print(f\"Categor√≠a: {example_chunk['category']}\")\n",
    "    print(f\"Source: {example_chunk['source']}\")\n",
    "    print(f\"Tags: {example_chunk['tags']}\")\n",
    "    print(f\"\\nContenido (primeros 300 caracteres):\")\n",
    "    print(f\"{example_chunk['content'][:300]}...\")\n",
    "    print(f\"\\nEmbedding (primeros 5 valores): {example_chunk['embedding'][:5]}\")\n",
    "    print(f\"Dimensi√≥n del embedding: {len(example_chunk['embedding'])}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No hay chunks disponibles para mostrar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
