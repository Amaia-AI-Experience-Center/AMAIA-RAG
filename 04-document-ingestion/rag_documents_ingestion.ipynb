{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2e817a0",
   "metadata": {},
   "source": [
    "# Pipeline de Ingesta de Documentos\n",
    "## Ingesta de Documentos PDF con Embeddings y Tags Automáticos\n",
    "\n",
    "Este notebook implementa un pipeline completo para:\n",
    "1. Extraer texto de archivos PDF\n",
    "2. Dividir en chunks procesables\n",
    "3. Generar embeddings con Azure OpenAI\n",
    "4. Clasificar automáticamente con tags relevantes usando LLM\n",
    "5. Guardar metadatos enriquecidos en JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87863851",
   "metadata": {},
   "source": [
    "## 1. Configuración de Variables de Entorno y Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656f1c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "from openai import AzureOpenAI\n",
    "import pymupdf4llm\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Initialize Azure OpenAI client for chat (US endpoint)\n",
    "chat_client = AzureOpenAI(\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY_US\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT_US\"]\n",
    ")\n",
    "\n",
    "# Initialize Azure OpenAI client for embeddings (default endpoint)\n",
    "embeddings_client = AzureOpenAI(\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    ")\n",
    "\n",
    "CHAT_DEPLOYMENT = os.environ[\"AZURE_OPENAI_DEPLOYMENT_CHAT\"]\n",
    "EMBEDDING_DEPLOYMENT = os.environ[\"AZURE_OPENAI_DEPLOYMENT_EMBEDDING\"]\n",
    "\n",
    "# Define directories\n",
    "data_dir = pathlib.Path.cwd() / \"data\"\n",
    "data_dir.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29bc4c7",
   "metadata": {},
   "source": [
    "## 2. Cargar y Extraer Texto de Archivos PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97acdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all PDF files in data directory\n",
    "filenames = [f.name for f in data_dir.glob(\"*.pdf\")]\n",
    "\n",
    "print(f\"\\nArchivos PDF encontrados: {len(filenames)}\")\n",
    "for i, filename in enumerate(filenames, 1):\n",
    "    print(f\"   {i}. {filename}\")\n",
    "\n",
    "# Dictionary to store extracted text from each file\n",
    "extracted_documents = {}\n",
    "\n",
    "for filename in filenames:\n",
    "    file_path = data_dir / filename\n",
    "    print(f\"\\nProcesando: {filename}\")\n",
    "    \n",
    "    try:\n",
    "        # Extract text from PDF to markdown format\n",
    "        md_text = pymupdf4llm.to_markdown(file_path)\n",
    "        extracted_documents[filename] = md_text\n",
    "        \n",
    "        print(f\"   Texto extraído: {len(md_text)} caracteres\")\n",
    "        print(f\"   Preview: {md_text[:150]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error al procesar: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bfbdd2",
   "metadata": {},
   "source": [
    "## 3. Dividir Texto en Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984661cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4o\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=125\n",
    ")\n",
    "\n",
    "# Store all chunks\n",
    "all_chunks_raw = {}\n",
    "\n",
    "print(f\"\\nDividiendo documentos en chunks...\")\n",
    "for filename, md_text in extracted_documents.items():\n",
    "    texts = text_splitter.create_documents([md_text])\n",
    "    all_chunks_raw[filename] = texts\n",
    "    \n",
    "    print(f\"   {filename}: {len(texts)} chunks\")\n",
    "    \n",
    "    # Show first chunk as example\n",
    "    if texts:\n",
    "        print(f\"      Ejemplo - Chunk 1: {texts[0].page_content[:100]}...\")\n",
    "\n",
    "print(f\"\\nChunking completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a368e",
   "metadata": {},
   "source": [
    "## 4. Clasificar Chunks con LLM y Extraer Tags\n",
    "\n",
    "Se usará un modelo LLM para analizar automáticamente cada chunk y extraer tags relevantes de la lista predefinida.\n",
    "\n",
    "**Tags disponibles:**\n",
    "- introduccion\n",
    "- barrios\n",
    "- gastronomia\n",
    "- museos\n",
    "- eventos\n",
    "- naturaleza\n",
    "- vida_nocturna\n",
    "- transporte\n",
    "- excursiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815544da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define available tags\n",
    "AVAILABLE_TAGS = [\n",
    "    \"introduccion\",\n",
    "    \"barrios\",\n",
    "    \"gastronomia\",\n",
    "    \"museos\",\n",
    "    \"eventos\",\n",
    "    \"naturaleza\",\n",
    "    \"vida_nocturna\",\n",
    "    \"transporte\",\n",
    "    \"excursiones\"\n",
    "]\n",
    "\n",
    "def extract_tags_from_chunk(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Use an LLM to extract relevant tags from chunk content.\n",
    "    \n",
    "    Args:\n",
    "        text: Chunk content\n",
    "    \n",
    "    Returns:\n",
    "        List of relevant tags\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"Analiza el siguiente texto y AÑADE SI SE MENCIONA algo relacionado con uno o mas de estos tags: \n",
    "{', '.join(AVAILABLE_TAGS)}\n",
    "\n",
    "Texto: {text[:1000]}\n",
    "\n",
    "Responde SOLO con una lista de las tags mencionadas en el texto en formato JSON, ejemplo: [\"tag1\", \"tag2\"]\n",
    "Sin texto adicional, solo el JSON.\"\"\"\n",
    "\n",
    "        response = chat_client.chat.completions.create(\n",
    "            model=CHAT_DEPLOYMENT,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        # Parse JSON response\n",
    "        response_text = response.choices[0].message.content.strip()\n",
    "        tags = json.loads(response_text)\n",
    "        \n",
    "        # Validate that all tags are in the allowed list\n",
    "        valid_tags = [tag for tag in tags if tag in AVAILABLE_TAGS]\n",
    "        return valid_tags if valid_tags else [\"\"]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Error al extraer tags: {e}\")\n",
    "        return [\"\"]\n",
    "\n",
    "print(\"Función de extracción de tags definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbddccf",
   "metadata": {},
   "source": [
    "## 5. Generar Embeddings con Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789109ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Generate an embedding for a text using Azure OpenAI.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to generate embedding for\n",
    "    \n",
    "    Returns:\n",
    "        Embedding vector (list of floats)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = embeddings_client.embeddings.create(\n",
    "            model=EMBEDDING_DEPLOYMENT,\n",
    "            input=text\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"   Error al generar embedding: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"Función de generación de embeddings definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb5b50",
   "metadata": {},
   "source": [
    "## 6. Estructurar Datos con Metadatos\n",
    "\n",
    "Crear la estructura completa con: id, content, category (nombre del archivo), source, tags, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58acf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = []\n",
    "chunk_counter = 1\n",
    "\n",
    "print(f\"\\nProcesando chunks con metadatos...\")\n",
    "\n",
    "for filename, chunks in all_chunks_raw.items():\n",
    "    print(f\"   {filename}\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        try:\n",
    "            text_content = chunk.page_content\n",
    "            \n",
    "            # Extract tags using LLM\n",
    "            print(f\"      Chunk {i}/{len(chunks)}: extrayendo tags...\", end=\"\")\n",
    "            tags = extract_tags_from_chunk(text_content)\n",
    "            print(f\" {tags}\")\n",
    "            \n",
    "            # Generate embedding\n",
    "            print(f\"      Chunk {i}/{len(chunks)}: generando embedding...\", end=\"\")\n",
    "            embedding = generate_embedding(text_content)\n",
    "            print(f\" OK\")\n",
    "            \n",
    "            # Create chunk structure with all metadata\n",
    "            chunk_data = {\n",
    "                \"id\": f\"{filename.replace('.', '_')}-{i}\",\n",
    "                \"content\": text_content,\n",
    "                \"category\": filename.replace(\".pdf\", \"\"),  # Filename without extension\n",
    "                \"source\": \"pdf_ingestion\",  # Document source\n",
    "                \"tags\": tags,\n",
    "                \"embedding\": embedding\n",
    "            }\n",
    "            \n",
    "            all_chunks.append(chunk_data)\n",
    "            chunk_counter += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      Error al procesar chunk {i}: {e}\")\n",
    "\n",
    "print(f\"\\nProcesamiento completado: {len(all_chunks)} chunks generados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2453db7a",
   "metadata": {},
   "source": [
    "## 7. Guardar Chunks Procesados en JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc330752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed chunks to a JSON file\n",
    "output_file = data_dir / \"rag_ingested_chunks.json\"\n",
    "\n",
    "print(f\"\\nGuardando chunks en {output_file}...\")\n",
    "\n",
    "try:\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_chunks, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Archivo guardado exitosamente\")\n",
    "    print(f\"Total de chunks: {len(all_chunks)}\")\n",
    "    print(f\"Tamaño del archivo: {output_file.stat().st_size / 1024:.2f} KB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error al guardar archivo: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b236e",
   "metadata": {},
   "source": [
    "## Visualización de Resultados\n",
    "\n",
    "Revisar ejemplos de chunks procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Show summary information\n",
    "print(\"Resumen de Chunks Procesados:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if all_chunks:\n",
    "    # Create dataframe for analysis\n",
    "    df_summary = pd.DataFrame([\n",
    "        {\n",
    "            \"ID\": chunk[\"id\"],\n",
    "            \"Categoría\": chunk[\"category\"],\n",
    "            \"Tags\": \", \".join(chunk[\"tags\"]),\n",
    "            \"Longitud\": len(chunk[\"content\"]),\n",
    "            \"Has Embedding\": len(chunk[\"embedding\"]) > 0\n",
    "        }\n",
    "        for chunk in all_chunks[:10]  # Show first 10\n",
    "    ])\n",
    "    \n",
    "    print(df_summary.to_string(index=False))\n",
    "    \n",
    "    if len(all_chunks) > 10:\n",
    "        print(f\"\\n... y {len(all_chunks) - 10} chunks más\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Total de chunks: {len(all_chunks)}\")\n",
    "\n",
    "else:\n",
    "    print(\"No hay chunks para mostrar. Asegúrate de que hay archivos PDF en el directorio /data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb15a7",
   "metadata": {},
   "source": [
    "## Ejemplo de Chunk Completo\n",
    "\n",
    "Visualizar la estructura detallada de un chunk procesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8436e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_chunks:\n",
    "    # Show first chunk as example\n",
    "    example_chunk = all_chunks[0]\n",
    "    \n",
    "    print(\"Ejemplo de Chunk Procesado:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ID: {example_chunk['id']}\")\n",
    "    print(f\"Categoría: {example_chunk['category']}\")\n",
    "    print(f\"Source: {example_chunk['source']}\")\n",
    "    print(f\"Tags: {example_chunk['tags']}\")\n",
    "    print(f\"\\nContenido (primeros 300 caracteres):\")\n",
    "    print(f\"{example_chunk['content'][:300]}...\")\n",
    "    print(f\"\\nEmbedding (primeros 5 valores): {example_chunk['embedding'][:5]}\")\n",
    "    print(f\"Dimensión del embedding: {len(example_chunk['embedding'])}\")\n",
    "else:\n",
    "    print(\"No hay chunks disponibles para mostrar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
