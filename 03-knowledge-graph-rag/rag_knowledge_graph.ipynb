{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c651213",
   "metadata": {},
   "source": "# Sistema RAG con Grafo de Conocimiento\n\nEste notebook implementa un sistema de Generación Aumentada por Recuperación (RAG) usando un grafo de conocimiento pre-construido para responder preguntas sobre lenguajes de programación, frameworks y conceptos tecnológicos.\n\n## Importar Librerías Requeridas\n\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9278ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import networkx as nx\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39af2e24",
   "metadata": {},
   "source": "## Configurar Variables de Entorno"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b34425",
   "metadata": {},
   "outputs": [],
   "source": "load_dotenv(override=True)\n\n# Azure OpenAI API\nfrom openai import AzureOpenAI\n\nclient = AzureOpenAI(\n    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n)\nCHAT_DEPLOYMENT = os.environ[\"AZURE_OPENAI_DEPLOYMENT_CHAT\"]\n\nprint(f\"Modelo configurado: {CHAT_DEPLOYMENT}\")"
  },
  {
   "cell_type": "markdown",
   "id": "519247bb",
   "metadata": {},
   "source": "## Cargar Grafo de Conocimiento Serializado\n\nCargar el grafo de conocimiento pre-construido desde disco para acceso instantáneo. El grafo contiene entidades y relaciones sobre conceptos tecnológicos."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788211ad",
   "metadata": {},
   "outputs": [],
   "source": "# Load graph from pickle\ngraph_pickle_path = Path(\"data/knowledge_graph.pkl\")\n\nprint(\"Cargando grafo serializado...\")\nwith open(graph_pickle_path, 'rb') as f:\n    graph = pickle.load(f)\n\n# Extract entity dictionary\nentities = graph.graph['entities']\n\nprint(f\"Grafo cargado:\")\nprint(f\"  - Entidades: {graph.number_of_nodes()}\")\nprint(f\"  - Relaciones: {graph.number_of_edges()}\")\nprint(f\"  - Tipos de entidad: {set(data['type'] for _, data in graph.nodes(data=True))}\")"
  },
  {
   "cell_type": "markdown",
   "id": "02138353",
   "metadata": {},
   "source": "## Definir Funciones de Búsqueda y Contexto\n\nImplementar búsqueda de entidades con puntuación basada en nombre, tipo y propiedades. También definir funciones para recuperar el contexto de entidades incluyendo relaciones."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b859b8ac",
   "metadata": {},
   "outputs": [],
   "source": "def search_entities(query: str, top_k: int = 5) -> List[Dict]:\n    \"\"\"Search entities by name or properties.\"\"\"\n    query_lower = query.lower()\n    query_tokens = [token for token in re.split(r\"\\W+\", query_lower) if token]\n    \n    if not query_tokens:\n        return []\n    \n    matches = []\n    for entity_id, entity in entities.items():\n        score = 0\n        \n        # Name matching\n        entity_name = entity['name'].lower()\n        name_matches = sum(1 for token in query_tokens if token in entity_name)\n        score += 10 * name_matches\n        \n        # Type matching\n        entity_type = entity['type'].lower()\n        type_matches = sum(1 for token in query_tokens if token in entity_type)\n        score += 5 * type_matches\n        \n        # Property matching\n        for key, value in entity.get('properties', {}).items():\n            key_lower = str(key).lower()\n            value_lower = str(value).lower()\n            prop_matches = sum(\n                1 for token in query_tokens \n                if token in key_lower or token in value_lower\n            )\n            score += 3 * prop_matches\n        \n        if score > 0:\n            matches.append({'entity': entity, 'score': score})\n    \n    matches.sort(key=lambda x: x['score'], reverse=True)\n    return [m['entity'] for m in matches[:top_k]]\n\n\ndef get_entity_context(entity_id: str) -> Dict:\n    \"\"\"Get entity and its immediate relationships.\"\"\"\n    if entity_id not in entities:\n        return None\n    \n    context = {\n        'entity': entities[entity_id],\n        'relationships': []\n    }\n    \n    # Outgoing relationships\n    for target in graph.successors(entity_id):\n        edge_data = graph.get_edge_data(entity_id, target)\n        context['relationships'].append({\n            'type': edge_data['rel_type'],\n            'direction': 'outgoing',\n            'target': entities[target],\n            'properties': edge_data.get('properties', {})\n        })\n    \n    # Incoming relationships\n    for source in graph.predecessors(entity_id):\n        edge_data = graph.get_edge_data(source, entity_id)\n        context['relationships'].append({\n            'type': edge_data['rel_type'],\n            'direction': 'incoming',\n            'source': entities[source],\n            'properties': edge_data.get('properties', {})\n        })\n    \n    return context\n\n\ndef format_context_for_llm(contexts: List[Dict]) -> str:\n    \"\"\"Format graph contexts for LLM consumption.\"\"\"\n    if not contexts:\n        return \"No se encontró información relevante en el grafo de conocimiento.\"\n    \n    formatted = \"Información del Grafo de Conocimiento:\\n\\n\"\n    \n    for i, ctx in enumerate(contexts, 1):\n        entity = ctx['entity']\n        formatted += f\"Entidad #{i}: {entity['name']} ({entity['type']})\\n\"\n        \n        # Properties\n        if entity.get('properties'):\n            formatted += \"  Propiedades:\\n\"\n            for key, value in entity['properties'].items():\n                formatted += f\"    - {key}: {value}\\n\"\n        \n        # Relationships\n        if ctx['relationships']:\n            formatted += \"  Relaciones:\\n\"\n            for rel in ctx['relationships']:\n                if rel['direction'] == 'outgoing':\n                    formatted += f\"    - {rel['type']} -> {rel['target']['name']} ({rel['target']['type']})\\n\"\n                else:\n                    formatted += f\"    - {rel['type']} <- {rel['source']['name']} ({rel['source']['type']})\\n\"\n                \n                # Relationship properties\n                if rel.get('properties'):\n                    for key, value in rel['properties'].items():\n                        formatted += f\"      {key}: {value}\\n\"\n        \n        formatted += \"\\n\"\n    \n    return formatted\n\nprint(\"Funciones de búsqueda definidas\")"
  },
  {
   "cell_type": "markdown",
   "id": "905cbbe0",
   "metadata": {},
   "source": "## Definir Mensajes de Sistema para el LLM"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56b2441",
   "metadata": {},
   "outputs": [],
   "source": "QUERY_REWRITE_SYSTEM_MESSAGE = \"\"\"\nEres un asistente útil que reescribe preguntas de usuarios en consultas de palabras clave\npara buscar en un grafo de conocimiento sobre tecnología, lenguajes de programación, frameworks y conceptos.\n\nExtrae las entidades clave, conceptos o temas sobre los que pregunta el usuario.\nEnfócate en nombres específicos de lenguajes, frameworks, librerías, organizaciones o conceptos técnicos.\n\nResponde SOLO con la consulta de palabras clave (2-6 palabras).\n\"\"\"\n\nSYSTEM_MESSAGE = \"\"\"\nEres un asistente útil que responde preguntas usando un grafo de conocimiento sobre\ntecnología, lenguajes de programación, frameworks, librerías y conceptos relacionados.\n\nDebes basar tus respuestas en los datos del grafo de conocimiento proporcionados en el contexto.\nSi la información no está en el grafo de conocimiento, indícalo claramente.\nUsa las relaciones y propiedades para proporcionar respuestas completas y precisas.\n\"\"\""
  },
  {
   "cell_type": "markdown",
   "id": "121611db",
   "metadata": {},
   "source": "## Función de Respuesta a Preguntas\n\nDefinir una función para procesar preguntas individuales a través del pipeline RAG: reescritura de consulta, búsqueda de entidades, recuperación de contexto y generación de respuesta."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b78f97",
   "metadata": {},
   "outputs": [],
   "source": "def ask_question(question: str) -> str:\n    \"\"\"Ask a single question to the RAG system.\"\"\"\n    messages_local = [{\"role\": \"system\", \"content\": SYSTEM_MESSAGE}]\n    \n    # Rewrite query\n    response = client.chat.completions.create(\n        model=CHAT_DEPLOYMENT,\n        temperature=0.05,\n        messages=[\n            {\"role\": \"system\", \"content\": QUERY_REWRITE_SYSTEM_MESSAGE},\n            {\"role\": \"user\", \"content\": f\"Nueva pregunta del usuario: {question}\"},\n        ],\n    )\n    search_query = response.choices[0].message.content\n    \n    # Search and get context\n    found_entities = search_entities(search_query, top_k=5)\n    \n    if found_entities:\n        contexts = [get_entity_context(e['id']) for e in found_entities if get_entity_context(e['id'])]\n        graph_context = format_context_for_llm(contexts)\n    else:\n        graph_context = \"No se encontró información relevante.\"\n    \n    print(f\"\\n[Consulta de búsqueda]:\\n{search_query}\")\n    print(f\"\\n[Contexto]:\\n{graph_context}\")\n\n    # Generate answer\n    messages_local.append({\n        \"role\": \"user\",\n        \"content\": f\"{question}\\n\\nContexto:\\n{graph_context}\"\n    })\n    \n    response = client.chat.completions.create(\n        model=CHAT_DEPLOYMENT,\n        temperature=0.3,\n        messages=messages_local\n    )\n    \n    return response.choices[0].message.content"
  },
  {
   "cell_type": "markdown",
   "id": "7f29ecbe",
   "metadata": {},
   "source": [
    "## Ejemplos de prompts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effc73e7",
   "metadata": {},
   "outputs": [],
   "source": "# Example usage: Query about Python's use cases\nanswer = ask_question(\"¿Para qué se usa Python?\")\nprint(f\"\\n[Respuesta]:\\n{answer}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb851c",
   "metadata": {},
   "outputs": [],
   "source": "# Example usage: Compare web frameworks\nanswer = ask_question(\"Háblame sobre GPT\")\nprint(f\"\\n[Respuesta]:\\n{answer}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174882c3",
   "metadata": {},
   "outputs": [],
   "source": "# Example usage: Query about relationships between technologies\nanswer = ask_question(\"¿Qué frameworks están construidos con JavaScript y para qué se usan?\")\nprint(f\"\\n[Respuesta]:\\n{answer}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}