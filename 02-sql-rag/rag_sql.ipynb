{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2218a19b",
   "metadata": {},
   "source": [
    "# RAG con SQL (Text-to-SQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d794e32",
   "metadata": {},
   "source": [
    "Configuración inicial: carga de variables de entorno y cliente OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c968ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "from pathlib import Path\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY_US\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT_US\"]\n",
    ")\n",
    "CHAT_DEPLOYMENT = os.environ[\"AZURE_OPENAI_DEPLOYMENT_CHAT\"]\n",
    "\n",
    "print(f\"Usando modelo: {CHAT_DEPLOYMENT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb4be2",
   "metadata": {},
   "source": [
    "Conexión a la base de datos SQLite y extracción del esquema de la tabla para informar al LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eaa5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "# We check relative path assuming notebook is in 02-sql-rag/\n",
    "db_path = Path(\"data/hybrid_cars.db\")\n",
    "if not db_path.exists():\n",
    "    # Fallback for absolute path if running from root context\n",
    "    db_path = Path(\"02-sql-rag/data/hybrid_cars.db\")\n",
    "\n",
    "print(f\"Usando base de datos: {db_path.absolute()}\")\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get database schema for the LLM\n",
    "cursor.execute(\"PRAGMA table_info(cars)\")\n",
    "schema_info = cursor.fetchall()\n",
    "schema_description = \"Tabla: cars\\nColumnas:\\n\"\n",
    "for col in schema_info:\n",
    "    schema_description += f\"  - {col[1]} ({col[2]}): \"\n",
    "    if col[1] == \"vehicle\":\n",
    "        schema_description += \"Nombre del modelo de coche\"\n",
    "    elif col[1] == \"year\":\n",
    "        schema_description += \"Año de fabricación\"\n",
    "    elif col[1] == \"msrp\":\n",
    "        schema_description += \"Precio de venta sugerido por el fabricante en dólares\"\n",
    "    elif col[1] == \"acceleration\":\n",
    "        schema_description += \"Tiempo 0-60 mph en segundos (menor es más rápido)\"\n",
    "    elif col[1] == \"mpg\":\n",
    "        schema_description += \"Millas por galón (mayor es más eficiente)\"\n",
    "    elif col[1] == \"class\":\n",
    "        schema_description += \"Clase de vehículo (Compacto, Mediano, SUV, etc.)\"\n",
    "    schema_description += \"\\n\"\n",
    "\n",
    "print(\"\\nEsquema de la base de datos:\")\n",
    "print(schema_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad6c5ce",
   "metadata": {},
   "source": [
    "Definición de los mensajes del sistema (prompts) para la generación de SQL y la respuesta en lenguaje natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9df5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System message for SQL generation\n",
    "SQL_GENERATION_SYSTEM_MESSAGE = f\"\"\"\n",
    "Eres un asistente experto en SQL. Tu tarea es convertir preguntas en lenguaje natural en consultas SQLite válidas.\n",
    "\n",
    "Esquema de la base de datos:\n",
    "{schema_description}\n",
    "\n",
    "Reglas importantes:\n",
    "1. Genera SOLO la consulta SQL, sin explicaciones ni formato markdown\n",
    "2. Usa sintaxis SQLite apropiada\n",
    "3. Usa siempre sentencias SELECT (no INSERT, UPDATE, DELETE, DROP)\n",
    "4. Usa cláusulas WHERE, ORDER BY, LIMIT según sea necesario\n",
    "5. Para preguntas sobre \"más rápido\", usa ORDER BY acceleration ASC (menor es más rápido)\n",
    "6. Para preguntas sobre \"más lento\", usa ORDER BY acceleration DESC\n",
    "7. Para \"más eficiente\" o \"mejor mpg\", usa ORDER BY mpg DESC\n",
    "8. Para \"más barato\", usa ORDER BY msrp ASC\n",
    "9. Para \"más caro\", usa ORDER BY msrp DESC\n",
    "10. Si se pregunta por un resultado único o \"el\", añade LIMIT 1\n",
    "11. Usa LIKE para coincidencias parciales (ej., WHERE vehicle LIKE '%Prius%')\n",
    "12. Considera el contexto de la conversación para refinar las consultas\n",
    "\n",
    "Ejemplos:\n",
    "P: \"¿qué tan rápido es el prius v?\"\n",
    "R: SELECT vehicle, acceleration, year, mpg FROM cars WHERE vehicle LIKE '%Prius V%'\n",
    "\n",
    "P: \"¿cuál es el coche más rápido?\"\n",
    "R: SELECT vehicle, acceleration, year, msrp FROM cars ORDER BY acceleration ASC LIMIT 1\n",
    "\n",
    "P: \"muéstrame coches eficientes\"\n",
    "R: SELECT vehicle, mpg, year, class FROM cars WHERE mpg > 40 ORDER BY mpg DESC\n",
    "\n",
    "P: \"dame la pickup más barata\"\n",
    "R: SELECT vehicle, msrp, year, mpg FROM cars WHERE class = 'Pickup Truck' ORDER BY msrp ASC LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "ANSWER_SYSTEM_MESSAGE = \"\"\"\n",
    "Eres un asistente útil que responde preguntas sobre coches basándote en una base de datos de coches híbridos.\n",
    "Debes usar los datos de los resultados de la consulta para responder las preguntas con precisión.\n",
    "Sé conciso y directo en tus respuestas. Usa el historial de la conversación para proporcionar respuestas contextuales.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a119f71e",
   "metadata": {},
   "source": [
    "Función principal que orquesta el flujo: pregunta -> SQL -> ejecución -> respuesta natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658465ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question(question, chat_history=None):\n",
    "    if chat_history is None:\n",
    "        chat_history = [{\"role\": \"system\", \"content\": ANSWER_SYSTEM_MESSAGE}]\n",
    "    \n",
    "    print(f\"Pregunta: {question}\")\n",
    "    \n",
    "    try:\n",
    "        # Generate SQL query using the LLM with conversation context\n",
    "        sql_response = client.chat.completions.create(\n",
    "            model=CHAT_DEPLOYMENT,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SQL_GENERATION_SYSTEM_MESSAGE},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Nueva pregunta: {question}\\n\\nContexto de conversación: {chat_history[-4:] if len(chat_history) > 4 else chat_history}\",\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        sql_query = sql_response.choices[0].message.content.strip()\n",
    "        # Remove any markdown code blocks if present\n",
    "        if sql_query.startswith(\"```\"):\n",
    "            sql_query = sql_query.split(\"\\n\", 1)[1]\n",
    "            sql_query = sql_query.rsplit(\"```\", 1)[0]\n",
    "        sql_query = sql_query.strip()\n",
    "\n",
    "        print(f\"\\nSQL Generado: {sql_query}\")\n",
    "\n",
    "        # Execute the query\n",
    "        cursor.execute(sql_query)\n",
    "        results = cursor.fetchall()\n",
    "\n",
    "        # Get column names\n",
    "        column_names = [description[0] for description in cursor.description]\n",
    "\n",
    "        # Format results as markdown table\n",
    "        if results:\n",
    "            results_table = \" | \".join(column_names) + \"\\n\"\n",
    "            results_table += \" | \".join(\"---\" for _ in column_names) + \"\\n\"\n",
    "            for row in results[:10]:  # Limit to 10 rows for display\n",
    "                results_table += \" | \".join(str(val) if val is not None else \"N/A\" for val in row) + \"\\n\"\n",
    "\n",
    "            if len(results) > 10:\n",
    "                results_table += f\"\\n... y {len(results) - 10} filas más\"\n",
    "\n",
    "            print(f\"\\nResultados de la consulta ({len(results)} filas):\")\n",
    "            print(results_table)\n",
    "        else:\n",
    "            results_table = \"No se encontraron resultados.\"\n",
    "            print(f\"\\n{results_table}\")\n",
    "\n",
    "        # Use the results to generate a natural language response\n",
    "        chat_history.append({\"role\": \"user\", \"content\": f\"{question}\\n\\nResultados de la consulta:\\n{results_table}\"})\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=CHAT_DEPLOYMENT,  messages=chat_history\n",
    "        )\n",
    "\n",
    "        llm_response = response.choices[0].message.content\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": llm_response})\n",
    "\n",
    "        print(f\"\\nRespuesta:\")\n",
    "        print(llm_response)\n",
    "        \n",
    "        return chat_history\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"\\nError SQL: {e}\")\n",
    "        print(\"La consulta generada podría ser inválida.\")\n",
    "        return chat_history\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        return chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf2532",
   "metadata": {},
   "source": [
    "Inicialización del historial de conversación y ejecución de ejemplos de preguntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ac46f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation history\n",
    "history = [{\"role\": \"system\", \"content\": ANSWER_SYSTEM_MESSAGE}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e15afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 1\n",
    "history = process_question(\"¿cuál es el coche más caro de cada año?\", history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cd4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 2\n",
    "history = process_question(\"¿Qué coche tiene la mejor aceleración (menor tiempo 0-60)?\", history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb0276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 3\n",
    "history = process_question(\"Muéstrame los 3 SUVs más eficientes en consumo\", history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
