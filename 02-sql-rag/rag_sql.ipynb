{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2218a19b",
   "metadata": {},
   "source": [
    "# RAG con SQL (Text-to-SQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d794e32",
   "metadata": {},
   "source": [
    "Configuración inicial: carga de variables de entorno y cliente OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c968ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "from pathlib import Path\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "API_HOST = os.getenv(\"API_HOST\", \"github\")\n",
    "client = openai.OpenAI(base_url=\"https://models.github.ai/inference\", api_key=os.environ[\"GITHUB_TOKEN\"])\n",
    "MODEL_NAME = os.getenv(\"GITHUB_MODEL\", \"openai/gpt-4o\")\n",
    "\n",
    "print(f\"Using model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb4be2",
   "metadata": {},
   "source": [
    "Conexión a la base de datos SQLite y extracción del esquema de la tabla para informar al LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eaa5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "# We check relative path assuming notebook is in 02-sql-rag/\n",
    "db_path = Path(\"data/hybrid_cars.db\")\n",
    "if not db_path.exists():\n",
    "    # Fallback for absolute path if running from root context\n",
    "    db_path = Path(\"02-sql-rag/data/hybrid_cars.db\")\n",
    "\n",
    "print(f\"Using database at: {db_path.absolute()}\")\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get database schema for the LLM\n",
    "cursor.execute(\"PRAGMA table_info(cars)\")\n",
    "schema_info = cursor.fetchall()\n",
    "schema_description = \"Table: cars\\nColumns:\\n\"\n",
    "for col in schema_info:\n",
    "    schema_description += f\"  - {col[1]} ({col[2]}): \"\n",
    "    if col[1] == \"vehicle\":\n",
    "        schema_description += \"Name of the car model\"\n",
    "    elif col[1] == \"year\":\n",
    "        schema_description += \"Year of manufacture\"\n",
    "    elif col[1] == \"msrp\":\n",
    "        schema_description += \"Manufacturer's Suggested Retail Price in dollars\"\n",
    "    elif col[1] == \"acceleration\":\n",
    "        schema_description += \"0-60 mph time in seconds (lower is faster)\"\n",
    "    elif col[1] == \"mpg\":\n",
    "        schema_description += \"Miles per gallon (higher is more efficient)\"\n",
    "    elif col[1] == \"class\":\n",
    "        schema_description += \"Vehicle class (Compact, Midsize, SUV, etc.)\"\n",
    "    schema_description += \"\\n\"\n",
    "\n",
    "print(\"\\nDatabase Schema:\")\n",
    "print(schema_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad6c5ce",
   "metadata": {},
   "source": [
    "Definición de los mensajes del sistema (prompts) para la generación de SQL y la respuesta en lenguaje natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9df5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System message for SQL generation\n",
    "SQL_GENERATION_SYSTEM_MESSAGE = f\"\"\"\n",
    "You are a SQL expert assistant. Your task is to convert natural language questions into valid SQLite queries.\n",
    "\n",
    "Database schema:\n",
    "{schema_description}\n",
    "\n",
    "Important rules:\n",
    "1. Generate ONLY the SQL query, no explanations or markdown formatting\n",
    "2. Use proper SQLite syntax\n",
    "3. Always use SELECT statements (no INSERT, UPDATE, DELETE, DROP)\n",
    "4. Use appropriate WHERE, ORDER BY, LIMIT clauses as needed\n",
    "5. For \"fastest\" questions, use ORDER BY acceleration ASC (lower is faster)\n",
    "6. For \"slowest\" questions, use ORDER BY acceleration DESC\n",
    "7. For \"most efficient\" or \"best mpg\", use ORDER BY mpg DESC\n",
    "8. For \"cheapest\", use ORDER BY msrp ASC\n",
    "9. For \"most expensive\", use ORDER BY msrp DESC\n",
    "10. If asking for a single result or \"the\", add LIMIT 1\n",
    "11. Use LIKE for partial matches (e.g., WHERE vehicle LIKE '%Prius%')\n",
    "12. Consider conversation context to refine queries\n",
    "\n",
    "Examples:\n",
    "Q: \"how fast is the prius v?\"\n",
    "A: SELECT vehicle, acceleration, year, mpg FROM cars WHERE vehicle LIKE '%Prius V%'\n",
    "\n",
    "Q: \"what is the fastest car?\"\n",
    "A: SELECT vehicle, acceleration, year, msrp FROM cars ORDER BY acceleration ASC LIMIT 1\n",
    "\n",
    "Q: \"show me efficient cars\"\n",
    "A: SELECT vehicle, mpg, year, class FROM cars WHERE mpg > 40 ORDER BY mpg DESC\n",
    "\n",
    "Q: \"give me the cheapest pickup truck\"\n",
    "A: SELECT vehicle, msrp, year, mpg FROM cars WHERE class = 'Pickup Truck' ORDER BY msrp ASC LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "ANSWER_SYSTEM_MESSAGE = \"\"\"\n",
    "You are a helpful assistant that answers questions about cars based on a hybrid car database.\n",
    "You must use the data from the query results to answer the questions accurately.\n",
    "Be concise and direct in your answers. Use the conversation history to provide context-aware responses.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a119f71e",
   "metadata": {},
   "source": [
    "Función principal que orquesta el flujo: pregunta -> SQL -> ejecución -> respuesta natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658465ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question(question, chat_history=None):\n",
    "    if chat_history is None:\n",
    "        chat_history = [{\"role\": \"system\", \"content\": ANSWER_SYSTEM_MESSAGE}]\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    \n",
    "    try:\n",
    "        # Generate SQL query using the LLM with conversation context\n",
    "        sql_response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            temperature=0.1,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SQL_GENERATION_SYSTEM_MESSAGE},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"New question: {question}\\n\\nConversation context: {chat_history[-4:] if len(chat_history) > 4 else chat_history}\",\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        sql_query = sql_response.choices[0].message.content.strip()\n",
    "        # Remove any markdown code blocks if present\n",
    "        if sql_query.startswith(\"```\"):\n",
    "            sql_query = sql_query.split(\"\\n\", 1)[1]\n",
    "            sql_query = sql_query.rsplit(\"```\", 1)[0]\n",
    "        sql_query = sql_query.strip()\n",
    "\n",
    "        print(f\"\\nGenerated SQL: {sql_query}\")\n",
    "\n",
    "        # Execute the query\n",
    "        cursor.execute(sql_query)\n",
    "        results = cursor.fetchall()\n",
    "\n",
    "        # Get column names\n",
    "        column_names = [description[0] for description in cursor.description]\n",
    "\n",
    "        # Format results as markdown table\n",
    "        if results:\n",
    "            results_table = \" | \".join(column_names) + \"\\n\"\n",
    "            results_table += \" | \".join(\"---\" for _ in column_names) + \"\\n\"\n",
    "            for row in results[:10]:  # Limit to 10 rows for display\n",
    "                results_table += \" | \".join(str(val) if val is not None else \"N/A\" for val in row) + \"\\n\"\n",
    "\n",
    "            if len(results) > 10:\n",
    "                results_table += f\"\\n... and {len(results) - 10} more rows\"\n",
    "\n",
    "            print(f\"\\nQuery results ({len(results)} rows):\")\n",
    "            print(results_table)\n",
    "        else:\n",
    "            results_table = \"No results found.\"\n",
    "            print(f\"\\n{results_table}\")\n",
    "\n",
    "        # Use the results to generate a natural language response\n",
    "        chat_history.append({\"role\": \"user\", \"content\": f\"{question}\\n\\nQuery results:\\n{results_table}\"})\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME, temperature=0.3, messages=chat_history\n",
    "        )\n",
    "\n",
    "        llm_response = response.choices[0].message.content\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": llm_response})\n",
    "\n",
    "        print(f\"\\nResponse:\")\n",
    "        print(llm_response)\n",
    "        \n",
    "        return chat_history\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"\\nSQL Error: {e}\")\n",
    "        print(\"The generated query might be invalid.\")\n",
    "        return chat_history\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        return chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf2532",
   "metadata": {},
   "source": [
    "Inicialización del historial de conversación y ejecución de ejemplos de preguntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ac46f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation history\n",
    "history = [{\"role\": \"system\", \"content\": ANSWER_SYSTEM_MESSAGE}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e15afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 1\n",
    "history = process_question(\"whats the most expensive car for each year\", history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cd4847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 2\n",
    "history = process_question(\"Which car has the best acceleration (lowest 0-60 time)?\", history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb0276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 3\n",
    "history = process_question(\"Show me the top 3 most fuel efficient SUVs\", history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
